{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da062536",
   "metadata": {},
   "source": [
    "# utils\n",
    "> This notebook was generated from the following notebooks:\n",
    "\n",
    "- [_00_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_00_utils.ipynb)\n",
    "- [_01_files.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_01_files.ipynb)\n",
    "- [_02_slice.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_02_slice.ipynb)\n",
    "- [_03_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_03_directory.ipynb)\n",
    "- [_04_filter_matrix_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_04_filter_matrix_directory.ipynb)\n",
    "- [_05_guards.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_05_guards.ipynb)\n",
    "- [_06_torch_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_06_torch_utils.ipynb)\n",
    "- [_07_adata.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_07_adata.ipynb)\n",
    "- [_08_modules.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_08_modules.ipynb)\n",
    "- [_09_exp.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_09_exp.ipynb)\n",
    "- [_10_archive.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_10_archive.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a03098",
   "metadata": {},
   "source": [
    "## Utils\n",
    "> This notebook was generated from [_00_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_00_utils.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect, string\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from typing import List, Any, Optional, Callable, Union, Tuple, Iterable, Set, TypeAlias, Type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4390fa23",
   "metadata": {},
   "source": [
    "### Basic Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def isiter(val: Any) -> bool:    \n",
    "    return isinstance(val, Iterable)\n",
    "\n",
    "def allinstance(vals:Any, dtype:Union[Type, TypeAlias]=Any) -> bool:\n",
    "    return isiter(vals) and all(isinstance(i, dtype) for i in vals)\n",
    "\n",
    "def allsametype(vals:Any) -> bool:\n",
    "    if not isiter(vals) or len(vals) == 0: return True\n",
    "    dtype = type(vals[0])\n",
    "    return isiter(vals) and all(isinstance(i, dtype) for i in vals)\n",
    "\n",
    "def isin(val:Any, vals:Iterable) -> bool:\n",
    "    return val in vals\n",
    "\n",
    "def arein(vals:Iterable, refs:Iterable) -> bool:                                             \n",
    "    return isiter(vals) and isiter(refs) and all(isin(v, refs) for v in vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05701369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def str_just_alpha(s:str) -> str:\n",
    "    '''Filters a string for just alpha values'''\n",
    "    return ''.join(list(filter(str.isalpha, s)))\n",
    "\n",
    "def str_just_numeric(s:str) -> str:\n",
    "    '''Filters a string for just numeric values'''\n",
    "    return ''.join(list(filter(str.isnumeric, s)))\n",
    "\n",
    "def strip_punc(s:str) -> str:\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbe2e4b6",
   "metadata": {},
   "source": [
    "### Argument and Key-Word Argument Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_kwargs_for_func(fn: Callable, **kwargs:Optional[dict]):\n",
    "    params = inspect.signature(fn).parameters\n",
    "    return {k:v for k,v in kwargs.items() if k in params}\n",
    "\n",
    "def filter_kwargs_for_class(cls: Callable, **kwargs:Optional[dict]):\n",
    "    params = inspect.signature(cls.__init__).parameters\n",
    "    return {k:v for k,v in kwargs.items() if k in params}\n",
    "\n",
    "def wrangle_kwargs_for_func(\n",
    "    fn: Callable, \n",
    "    defaults: Optional[dict]=None,\n",
    "    **kwargs:Optional[dict]\n",
    ") -> dict:\n",
    "    # copy defaults\n",
    "    params = (defaults or {}).copy()\n",
    "    # update with kwargs of our function\n",
    "    params.update(kwargs or {})\n",
    "    # filter for only the params that other function accepts\n",
    "    params = filter_kwargs_for_func(fn, **params)\n",
    "    return params\n",
    "\n",
    "def wrangle_kwargs_for_class(\n",
    "    cls: Callable, \n",
    "    defaults: Optional[dict]=None,\n",
    "    **kwargs:Optional[dict]\n",
    ") -> dict:\n",
    "    # copy defaults\n",
    "    params = (defaults or {}).copy()\n",
    "    # update with kwargs of our class\n",
    "    params.update(kwargs or {})\n",
    "    # filter for only the params that other class accepts\n",
    "    params = filter_kwargs_for_class(cls, **params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646fbec",
   "metadata": {},
   "source": [
    "## Files\n",
    "> This notebook was generated from [_01_files.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_01_files.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d43a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, sys, pwd, atexit, tempfile, inspect, pathlib\n",
    "import requests, tarfile, gzip, shutil\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Optional, List, Union, Iterable, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedecf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.static import (\n",
    "    EXT_GZ, EXT_TAR, EXT_TAR_GZ,\n",
    ")\n",
    "\n",
    "from iza.types import (\n",
    "    PathType, PathLike\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb2fba6a",
   "metadata": {},
   "source": [
    "### PathLib Interoperabliilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a10f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_pathlib(path:Any) -> bool:\n",
    "    \"\"\"Check if the input is a path\"\"\"\n",
    "    return isinstance(path, PathType)\n",
    "\n",
    "def is_pathlike(path:Any) -> bool:\n",
    "    \"\"\"Check if the input is a path\"\"\"\n",
    "    return isinstance(path, (PathLike))\n",
    "\n",
    "def is_path(path:Any, existance:Optional[bool]) -> bool:\n",
    "    \"\"\"Check if the input is a path\"\"\"\n",
    "    exists_q = os.path.exists(os.path.expanduser(path))\n",
    "    return is_pathlike(path) and (exists_q if existance else True)\n",
    "\n",
    "def str_to_path(path:str) -> PathType:\n",
    "    \"\"\"Convert a string to a pathlib.Path object\"\"\"\n",
    "    return pathlib.Path(path)\n",
    "\n",
    "def path_to_str(path: PathType) -> str:\n",
    "    \"\"\"Convert a pathlib.Path object to a string\"\"\"\n",
    "    return os.fspath(path)\n",
    "\n",
    "def to_abs_expanded(dirname:Optional[PathLike]=None) -> PathType:\n",
    "    if dirname is None:\n",
    "        dirname = os.getcwd()\n",
    "\n",
    "    dirname = os.path.expanduser(dirname) \n",
    "    dirname = os.path.abspath(dirname) \n",
    "\n",
    "    if not is_pathlib(dirname):\n",
    "        dirname = pathlib.Path(dirname)\n",
    "\n",
    "    return dirname\n",
    "\n",
    "def sort_file_first(path: PathType):\n",
    "    return (not path.is_file(), path.name.lower())\n",
    "\n",
    "def sort_directory_first(path: PathType):\n",
    "    return (path.is_file(), path.name.lower())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ad35bbe",
   "metadata": {},
   "source": [
    "### User Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_user() -> str:\n",
    "    user = pwd.getpwuid(os.getuid())[0]\n",
    "    return user\n",
    "\n",
    "def collapse_user(path: str) -> str:\n",
    "    _, rest = path.split(get_user())    \n",
    "    return '~' + rest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0bf75cf",
   "metadata": {},
   "source": [
    "### Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_ext(filename:str, extension:str) -> bool:\n",
    "    if is_pathlib(filename):\n",
    "        filename = path_to_str(filename)\n",
    "    has_extension = extension in filename \n",
    "    splits = filename.split(extension)\n",
    "    is_end_of_str = len(splits) >= 2 and splits[-1] == ''\n",
    "    is_end_of_str = filename.endswith(extension)\n",
    "    return has_extension and is_end_of_str\n",
    "\n",
    "def drop_ext(filename:str, extension:Optional[str]=None) -> str:\n",
    "    if is_pathlib(filename):\n",
    "        filename = path_to_str(filename)\n",
    "        \n",
    "    file = os.path.basename(filename)\n",
    "    if extension is None:\n",
    "        file, *_ = file.split('.')\n",
    "    else:\n",
    "        file = filename.replace(extension, '')\n",
    "    return os.path.join(os.path.dirname(filename), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_tar(filename:str) -> bool:\n",
    "    return check_ext(filename, EXT_TAR)\n",
    "\n",
    "def is_gz(filename:str) -> bool:\n",
    "    return check_ext(filename, EXT_GZ)\n",
    "\n",
    "def is_targz(filename:str) -> bool:\n",
    "    return check_ext(filename, EXT_TAR_GZ)\n",
    "\n",
    "def is_tarball(filename:str) -> bool:\n",
    "    return is_tar(filename) or is_targz(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_for_gz_files(files:List[str]) -> List[str]:\n",
    "    return list(filter(lambda f: is_gz(f), files))\n",
    "\n",
    "def get_gz_files_in_dir(dirname:str) -> List[str]:\n",
    "    all_files = []\n",
    "\n",
    "    for (root, dirs, files) in os.walk(dirname):   \n",
    "        fullpaths = [os.path.join(root, file) for file in files]\n",
    "        all_files.extend(fullpaths)\n",
    "    \n",
    "    gz_files = filter_for_gz_files(all_files)\n",
    "    return gz_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6926b6d",
   "metadata": {},
   "source": [
    "### Decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c045279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decompress_tarball(filename:str) -> Tuple[str, Optional[EOFError]]:\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "        dirname : str\n",
    "            The name of the archive e.g. `~/Downloads/fluentbio.tar.gz` would\n",
    "            yield `~/Downloads/fluentbio`\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    FluentBio has a weird gzip so it complains when it is \n",
    "        actually fine\n",
    "    '''\n",
    "    error = None\n",
    "    decompress_dir = os.path.dirname(filename)\n",
    "    dirname = drop_ext(filename, EXT_TAR_GZ)\n",
    "    try:\n",
    "        with tarfile.open(filename) as tarball:\n",
    "            tarball.extractall(decompress_dir)\n",
    "            tarball.close()\n",
    "\n",
    "    except EOFError as error:\n",
    "        pass\n",
    "\n",
    "    return dirname, error\n",
    "\n",
    "\n",
    "def decompress_gunzip(filename:str, remove:bool=False) -> Tuple[str, Optional[EOFError]]:\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "        file : str\n",
    "            The name of the decompressed file e.g. `~/Downloads/fluentbio.tsv.gz` would\n",
    "            yield `~/Downloads/fluentbio.tsv`\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    FluentBio has a weird gzip so it complains when it is \n",
    "        actually fine\n",
    "    '''\n",
    "    error = None\n",
    "    decompressed_file = drop_ext(filename, EXT_GZ)\n",
    "    try:             \n",
    "        with gzip.open(filename, 'rb') as gunzipped:\n",
    "            with open(decompressed_file, 'wb') as unzipped:\n",
    "                shutil.copyfileobj(gunzipped, unzipped)     \n",
    "                   \n",
    "    except EOFError as error:\n",
    "        pass\n",
    "\n",
    "    if os.path.isfile(decompressed_file) and remove:\n",
    "        os.remove(filename)\n",
    "\n",
    "    return decompressed_file, error\n",
    "\n",
    "def undo_gz(filename: str) -> str:\n",
    "    if is_gz(filename):\n",
    "        filename, _ = decompress_gunzip(filename, remove=True)\n",
    "    elif is_tarball(filename):\n",
    "        filename, _ = decompress_tarball(filename)\n",
    "    return filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d64bf3ba",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_missing_dirs(dirs:List[str]):\n",
    "    if isinstance(dirs, str):\n",
    "        dirs = [dirs]\n",
    "        \n",
    "    elif is_pathlib(dirs):\n",
    "        dirs = [dirs]\n",
    "        \n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "            \n",
    "def dir_dirs(dirname:str) -> List[str]:\n",
    "    entries = os.listdir(dirname)\n",
    "    is_subdir = lambda e : os.path.isdir(os.path.join(dirname, e))\n",
    "    return list(filter(is_subdir, entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7397d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decompress_directory_of_gunzipped_files(\n",
    "    dirname:str, desc:Optional[str]=None, remove:Optional[bool]=False\n",
    ") -> None:\n",
    "    if desc is None:\n",
    "        desc = dirname.split('/')[-1]\n",
    "\n",
    "    gz_files = get_gz_files_in_dir(dirname)\n",
    "    for filename in tqdm(gz_files, desc=desc):\n",
    "        decomp_filename, error = decompress_gunzip(filename, remove)   \n",
    "\n",
    "\n",
    "def decompress_tarball_of_gunzipped_files(\n",
    "    filename:str, desc:Optional[str]=None, remove:Optional[bool]=False\n",
    ") -> None:\n",
    "    # NOTE: initial decompress of .tar.gz\n",
    "    dirname, error = decompress_tarball(filename)\n",
    "\n",
    "    if desc is None:\n",
    "        desc = dirname.split('/')[-1]\n",
    "\n",
    "    # NOTE: decompress all internal .gz files\n",
    "    decompress_directory_of_gunzipped_files(dirname, desc, remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e12ada0",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_file(uri:str, filename:Optional[str]=None, desc:Optional[str]=None) -> None:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    uri : str\n",
    "        The URI to download\n",
    "\n",
    "    filename : str, optional\n",
    "        The fullpath name of the file to download. Defaults to \n",
    "        `~/Downloads/os.path.basename(uri)`.\n",
    "\n",
    "    desc : str, optional\n",
    "        The description of the `tqdm` progress bar. Defaults to \n",
    "        `os.path.basename(uri)`.\n",
    "    '''\n",
    "    if filename is None:\n",
    "        download_dir = os.path.expanduser(f'~/Downloads')        \n",
    "        filename = os.path.join(download_dir, os.path.basename(uri))\n",
    "\n",
    "    basename = os.path.basename(filename)\n",
    "    if desc is None:\n",
    "        desc = basename\n",
    "\n",
    "    response = requests.get(uri, stream=True)\n",
    "    total = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with tqdm.wrapattr(\n",
    "        open(filename, 'wb'), 'write', \n",
    "        miniters=1, desc=desc, total=total\n",
    "    ) as fout:\n",
    "        for chunk in response.iter_content(chunk_size=4096):\n",
    "            fout.write(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fec6694",
   "metadata": {},
   "source": [
    "### Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_and_decompress_tarball_of_gunzipped_files(\n",
    "    uri:str, download_dir:str=None, desc:Optional[str]=None, remove:Optional[bool]=False\n",
    "):\n",
    "    filename = os.path.basename(uri)\n",
    "    fullpath = os.path.join(download_dir, filename)\n",
    "\n",
    "    if desc is None:\n",
    "        description = f'Downloading {filename}'\n",
    "\n",
    "\n",
    "    # NOTE: Amazon --> filtered_matrix.tar.gz\n",
    "    stream_file(uri, description)\n",
    "\n",
    "    if desc is None:\n",
    "        description = f'Decompressing {filename}'\n",
    "    # NOTE: filtered_matrix.tar.gz --> filtered_matrix/**/file.tsv\n",
    "    decompress_tarball_of_gunzipped_files(fullpath, desc, remove)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff26ed53",
   "metadata": {},
   "source": [
    "### Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_temp_file(**kwargs: Any) -> tempfile.NamedTemporaryFile:\n",
    "    temp = tempfile.NamedTemporaryFile(**kwargs)\n",
    "    @atexit.register\n",
    "    def delete_temp() -> None:\n",
    "        temp.close()\n",
    "    return temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "789d1da8",
   "metadata": {},
   "source": [
    "### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa502f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from urllib3.util.url import parse_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c185d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def urljoin(*parts: str) -> str:\n",
    "    return parse_url('/'.join(s.strip('/') for s in parts)).url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5191e5",
   "metadata": {},
   "source": [
    "## Slice\n",
    "> This notebook was generated from [_02_slice.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_02_slice.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46751753",
   "metadata": {},
   "source": [
    "### Slice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04704237",
   "metadata": {},
   "source": [
    "Helps convert `slices` to its numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eea43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Slice:\n",
    "    \"\"\"A class for representing a slice and providing conversion to other formats.\"\"\"\n",
    "    slc: slice = field(default_factory=slice)\n",
    "\n",
    "    @property\n",
    "    def start(self):\n",
    "        try:\n",
    "            return self._start\n",
    "        except AttributeError:\n",
    "            self._start = self.slc.start\n",
    "        return self._start\n",
    "    \n",
    "    @start.setter\n",
    "    def start(self, value):\n",
    "        \"\"\"Sets the start index.\"\"\"\n",
    "        if value < 0:\n",
    "            raise ValueError(\"Slice indices must be non-negative.\")\n",
    "        self._start = value\n",
    "\n",
    "    @property\n",
    "    def stop(self):\n",
    "        try:\n",
    "            return self._stop\n",
    "        except AttributeError:\n",
    "            self._stop = self.slc.stop\n",
    "        return self._stop\n",
    "    \n",
    "    @stop.setter\n",
    "    def stop(self, value):\n",
    "        \"\"\"Sets the stop index.\"\"\"\n",
    "        if value < 0:\n",
    "            raise ValueError(\"Slice indices must be non-negative.\")\n",
    "        self._stop = value\n",
    "    \n",
    "    @property\n",
    "    def step(self):\n",
    "        \"\"\"Gets the step index.\"\"\"\n",
    "        try:\n",
    "            return self._step\n",
    "        except AttributeError:\n",
    "            self._step = self.slc.step\n",
    "        return self._step\n",
    "    \n",
    "    @step.setter\n",
    "    def step(self, value):\n",
    "        \"\"\"Sets the step index.\"\"\"\n",
    "        if value < 0:\n",
    "            raise ValueError(\"Slice step must be non-negative.\")\n",
    "        self._step = value\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.start is None:\n",
    "            self.start = 0\n",
    "\n",
    "        if self.stop is None:\n",
    "            self.stop = min(0, self.start, max(1, self.start))\n",
    "\n",
    "        if self.step is None:\n",
    "            self.step = 1\n",
    "\n",
    "    def totuple(self) -> Tuple[Union[int, float, None], Union[int, float, None], Union[int, float, None]]:\n",
    "        \"\"\"Converts the slice to a tuple.\"\"\"\n",
    "        return (self.start, self.stop, self.step)\n",
    "    \n",
    "    def toslice(self) -> slice:\n",
    "        \"\"\"Converts the updated slice.\"\"\"\n",
    "        return slice(self.start, self.stop, self.step)\n",
    "    \n",
    "    def tolist(self) -> List[Union[int, float]]:\n",
    "        \"\"\"Converts the slice to a list.\"\"\"\n",
    "        return list(range(self.start, self.stop, self.step))\n",
    "    \n",
    "    def todict(self) -> List[Union[int, float]]:\n",
    "        \"\"\"Converts the slice to a dict.\"\"\"\n",
    "        return dict(zip('start stop step'.split(), self.totuple()))\n",
    "        \n",
    "\n",
    "    def astype(self, dtype:str):\n",
    "        \"\"\"Converts the slice to a specified format.\"\"\"\n",
    "        if dtype in {'list', list}:\n",
    "            return self.tolist()\n",
    "        elif dtype in {'numpy', np.ndarray}:\n",
    "            return np.array(self.tolist())\n",
    "        elif dtype in {'pandas', pd.Series}:\n",
    "            return pd.Series(self.tolist())\n",
    "        elif dtype in {'tuple', tuple}:\n",
    "            return self.totuple()\n",
    "        elif dtype in {'dict', dict}:\n",
    "            return self.todict()\n",
    "        elif dtype in {'slice', slice}:\n",
    "            return self.toslice()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63b630",
   "metadata": {},
   "source": [
    "## Director\n",
    "> This notebook was generated from [_03_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_03_directory.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64975cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, pathlib \n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar, Any, TypeAlias\n",
    "from enum import StrEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb96c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import PathLike, PathType\n",
    "from iza.static import EXT_PY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a607dbaf",
   "metadata": {},
   "source": [
    "### Directory Viewer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9909e536",
   "metadata": {},
   "source": [
    "#### Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DirectoryTreeStrings(StrEnum):\n",
    "    SPACE : ClassVar[str] = '    '\n",
    "    BRANCH: ClassVar[str] = 'â”‚   '    \n",
    "    TEE   : ClassVar[str] = 'â”œâ”€â”€ '\n",
    "    LAST  : ClassVar[str] = 'â””â”€â”€ '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ExtensionIcons(StrEnum):\n",
    "    PY: ClassVar[str] = \"ðŸ\"\n",
    "\n",
    "    @classmethod    \n",
    "    def get(cls, path:PathLike):\n",
    "        if isinstance(path, str): \n",
    "            path = Path(path)\n",
    "            if path.suffix == EXT_PY:\n",
    "                return cls[EXT_PY.lstrip('.').upper()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:    \n",
    "    from rich.tree import Tree\n",
    "    from rich.text import Text\n",
    "    from rich.markup import escape\n",
    "    from rich.filesize import decimal\n",
    "    from rich import get_console\n",
    "    from rich.console import Console\n",
    "    from rich.progress import Progress\n",
    "    TreeType = Tree\n",
    "    TextType = Text    \n",
    "    ConsoleType = Console\n",
    "    ProgressType = Progress\n",
    "\n",
    "    def rich_link_style(path:Path):\n",
    "        return f'link file://{path}'\n",
    "    \n",
    "    def rich_link_file(path: Path):\n",
    "        return f'[{rich_link_style(path)}]{escape(path.name)}[/]'\n",
    "    \n",
    "    def rich_file_size(path: Path):\n",
    "        size = path.stat().st_size\n",
    "        return decimal(size)\n",
    "    \n",
    "    def rich_file(path: Path):\n",
    "        size = rich_file_size(path)\n",
    "        text = Text(path.name)\n",
    "        text.stylize(rich_link_style(path))\n",
    "        text.append(f' ({size})', \"cyan\")\n",
    "        return text\n",
    "    \n",
    "except ImportError:\n",
    "    Tree = None\n",
    "    Text = None \n",
    "    TreeType = Any\n",
    "    TextType = Any\n",
    "    ConsoleType = Any\n",
    "    Progress = None\n",
    "    ProgressType = Any\n",
    "    \n",
    "    def rich_file(path: Path):\n",
    "        return path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f520f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dir_tree_entry(path:Path, pointer:Optional[str]='', prefix:Optional[str]='') -> str:\n",
    "    name = rich_file(path)\n",
    "    # NOTE: defined in _02_utils/_08_modules.ipynb\n",
    "    if is_rich_available():\n",
    "        return name\n",
    "    return f'{prefix}{pointer}{path.name}'\n",
    "\n",
    "#| export\n",
    "def try_init_rich_tree(dirname:PathLike) -> Optional[TreeType]:\n",
    "    if is_rich_available() and 'Tree' in globals():\n",
    "        return Tree(f'[link file://{dirname}]{dirname}', guide_style='bold bright_blue')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def walk_dir_tree(\n",
    "    dirname: PathLike, prefix: str = '',\n",
    "    hidden: Optional[bool] = False, tree: Optional[TreeType] = None,\n",
    "):    \n",
    "    '''\n",
    "    A recursive generator, given a directory Path object\n",
    "    will yield a visual tree structure line by line\n",
    "    with each line prefixed by the same characters\n",
    "    Notes\n",
    "    -----\n",
    "    Adapted from https://stackoverflow.com/a/59109706/5623899\n",
    "    '''\n",
    "    # NOTE: sort_directory_first, to_abs_expanded, rich_file defined in _02_utils/_08_modules.ipynb\n",
    "    dirname = Path(to_abs_expanded(dirname))\n",
    "    contents = sorted(Path(dirname).iterdir(), key=sort_directory_first)\n",
    "    \n",
    "    SPACE, BRANCH, TEE, LAST = DirectoryTreeStrings\n",
    "    pointers = [TEE] * (len(contents) - 1) + [LAST]\n",
    "    \n",
    "    for pointer, path in zip(pointers, contents):\n",
    "        # Remove hidden files\n",
    "        if path.name.startswith('.') and not hidden:\n",
    "            continue\n",
    "        \n",
    "        name = dir_tree_entry(path, pointer, prefix)\n",
    "        branch = None\n",
    "        if tree is not None:\n",
    "            branch = tree.add(rich_file(path))\n",
    "        yield name\n",
    "\n",
    "    \n",
    "        if path.is_dir():\n",
    "            # NOTE: space because last, â””â”€â”€ , above so no more |\n",
    "            extension = BRANCH if pointer == TEE else SPACE\n",
    "            yield from walk_dir_tree(path, prefix=f'{prefix}{extension}', hidden=hidden, tree=branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DirectoryTree:\n",
    "    dirname: str\n",
    "    hidden: Optional[bool] = False\n",
    "    def __post_init__(self):\n",
    "        # NOTE: defined in _02_utils/_01_files.ipynb\n",
    "        self.dirname = Path(to_abs_expanded(self.dirname))\n",
    "\n",
    "    def tree_generator(self, dirname: Optional[PathLike] = None):\n",
    "        dirname = getattr(self, 'dirname', dirname)\n",
    "        tree = getattr(self, 'tree', None)\n",
    "        yield from walk_dir_tree(self.dirname, prefix='', hidden=self.hidden, tree=tree)\n",
    "        self.tree = tree\n",
    "        \n",
    "    def get_tree_lines(self, dirname: Optional[PathLike] = None) -> List[str]:\n",
    "        dirname = getattr(self, 'dirname', dirname)\n",
    "        tree_gen = self.tree_generator(dirname)\n",
    "        lines = [line for line in tree_gen]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c261b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class RichTreeMixin(DirectoryTree):\n",
    "    console: Optional[ConsoleType] = field(default_factory=Console, init=False, repr=False)\n",
    "    tree: Optional[TreeType] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        if self.console is None and is_rich_available():\n",
    "            self.console = get_console()\n",
    "\n",
    "        if self.tree is None:\n",
    "            self.tree = try_init_rich_tree(self.dirname)\n",
    "\n",
    "    def print_rich(self, dirname: Optional[str] = None, tree: Optional[TreeType] = None) -> None:\n",
    "        tree = getattr(self, 'tree', tree)\n",
    "        if tree is None:\n",
    "            self.tree = try_init_rich_tree(self.dirname)\n",
    "\n",
    "        lines = self.get_tree_lines(dirname)\n",
    "        self.console.print(self.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@dataclass\n",
    "class Directory(RichTreeMixin):\n",
    "    dirname: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "\n",
    "    def make_tree_str(self, dirname: Optional[PathLike] = None) -> str:\n",
    "        dirname = getattr(self, 'dirname', dirname)\n",
    "        lines = self.get_tree_lines(dirname)\n",
    "        tree_str = '\\n'.join([str(dirname), *lines])\n",
    "        return tree_str\n",
    "   \n",
    "    def print(self, dirname:Optional[str]=None) -> None:\n",
    "        dirname = self.prepare_dirname(dirname)\n",
    "        tree_str = self.make_tree_str(dirname)        \n",
    "        print(tree_str)\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        dirname = getattr(self, 'dirname', None)\n",
    "        tree_str = self.make_tree_str(dirname)        \n",
    "        return tree_str        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbd0f6",
   "metadata": {},
   "source": [
    "## Filter matrix director\n",
    "> This notebook was generated from [_04_filter_matrix_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_04_filter_matrix_directory.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "210de4f5",
   "metadata": {},
   "source": [
    "### Filter Matrix Directory Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c19e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aba4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.static import (\n",
    "    ADATA, MATRIX, BARCODES, FEATURES, EXT_H5, EXT_MTX, EXT_TSV,\n",
    "    GENE_SYMBOL, ENSEMBL_ID\n",
    ")\n",
    "from iza.types import (\n",
    "    AnnData\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try: \n",
    "    import scanpy as sc, scprep\n",
    "\n",
    "    # NOTE: Directory defined in _02_utils/_02_directory.ipynb\n",
    "    @dataclass\n",
    "    class FilterMatrixDirectory(Directory):\n",
    "        _: KW_ONLY\n",
    "        ADATA_FILE: ClassVar[str] = f'{ADATA}{EXT_H5}'\n",
    "        MATRIX_FILE: ClassVar[str] = f'{MATRIX}{EXT_MTX}'\n",
    "        BARCODES_FILE: ClassVar[str] = f'{BARCODES}{EXT_TSV}'\n",
    "        FEATURES_FILE: ClassVar[str] = f'{FEATURES}{EXT_TSV}'\n",
    "        \n",
    "\n",
    "        def __post_init__(self):    \n",
    "            try:\n",
    "                if not self.has_adata:\n",
    "                    self.make_adata()\n",
    "            except Exception as err:\n",
    "                raise err\n",
    "\n",
    "        def __repr__(self):\n",
    "            base = os.path.basename(self.dirname)\n",
    "            srep = f'FilteredMatrix(valid: {self.is_valid}, adata: {self.has_adata})'        \n",
    "            srep += '\\n'\n",
    "            srep += super(FilterMatrixDirectory, self).__repr__()\n",
    "            return srep\n",
    "                    \n",
    "        @property\n",
    "        def adata_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.ADATA_FILE)\n",
    "\n",
    "        @property\n",
    "        def matrix_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.MATRIX_FILE)\n",
    "        \n",
    "        @property\n",
    "        def barcodes_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.BARCODES_FILE)\n",
    "        \n",
    "        @property\n",
    "        def features_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.FEATURES_FILE)\n",
    "\n",
    "        @property\n",
    "        def has_adata(self) -> bool:\n",
    "            return os.path.isfile(self.adata_filename)\n",
    "\n",
    "        @property\n",
    "        def has_matrix(self) -> bool:\n",
    "            return os.path.isfile(self.matrix_filename)\n",
    "\n",
    "        @property\n",
    "        def has_barcodes(self) -> bool:\n",
    "            return os.path.isfile(self.barcodes_filename)\n",
    "\n",
    "        @property\n",
    "        def has_features(self) -> bool:\n",
    "            return os.path.isfile(self.features_filename)\n",
    "\n",
    "        @property\n",
    "        def is_valid(self) -> bool:\n",
    "            return all([self.has_matrix, self.has_barcodes, self.has_features])\n",
    "\n",
    "        def make_adata(self) -> AnnData:\n",
    "            if self.has_adata:\n",
    "                return\n",
    "\n",
    "            steps = (FEATURES, BARCODES, MATRIX, 'combine', ADATA)\n",
    "            \n",
    "            desc = os.path.basename(self.dirname)\n",
    "\n",
    "            steps = tqdm(steps, desc=desc, leave=True)        \n",
    "            for step in steps:\n",
    "                steps.set_postfix(stage=step)\n",
    "                match step:\n",
    "                    case 'features':\n",
    "                        features = pd.read_csv(self.features_filename, sep='\\t', header=None)\n",
    "                        features.columns = [ENSEMBL_ID, GENE_SYMBOL, 'feature_type']\n",
    "                        features.index = pd.Series(features.ensembl_id.copy().values)\n",
    "\n",
    "                    case 'barcodes':\n",
    "                        barcodes = pd.read_csv(self.barcodes_filename, sep='\\t', header=None)\n",
    "                        barcodes.columns = [BARCODES]\n",
    "                        barcodes.index = pd.Series(barcodes.barcodes.copy().values)\n",
    "\n",
    "                    case 'matrix':\n",
    "                        matrix = scprep.io.load_mtx(self.matrix_filename, sparse=True).T\n",
    "\n",
    "                    case 'combine':\n",
    "                        data = pd.DataFrame.sparse.from_spmatrix(\n",
    "                            matrix, columns=features.index, index = barcodes.index\n",
    "                        )\n",
    "                        del matrix\n",
    "\n",
    "                    case 'adata':\n",
    "                        adata = sc.AnnData(X=data.values, obs=barcodes, var=features, dtype='float32')\n",
    "                        adata.write(self.adata_filename)\n",
    "\n",
    "                    case _:\n",
    "                        pass\n",
    "\n",
    "            return adata\n",
    "\n",
    "        def get_adata(self) -> AnnData:\n",
    "            adata = sc.read_h5ad(self.adata_filename)\n",
    "            return adata\n",
    "\n",
    "except ImportError as err:\n",
    "    @dataclass\n",
    "    class FilterMatrixDirectory(Directory):\n",
    "        _: KW_ONLY\n",
    "        ADATA_FILE: ClassVar[str] = f'{ADATA}{EXT_H5}'\n",
    "        MATRIX_FILE: ClassVar[str] = f'{MATRIX}{EXT_MTX}'\n",
    "        BARCODES_FILE: ClassVar[str] = f'{BARCODES}{EXT_TSV}'\n",
    "        FEATURES_FILE: ClassVar[str] = f'{FEATURES}{EXT_TSV}'\n",
    "        \n",
    "\n",
    "        def __post_init__(self):    \n",
    "            raise ImportError('FilterMatrixDirectory requires scprep and scanpy to be installed')\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3fc1f",
   "metadata": {},
   "source": [
    "## Guards\n",
    "> This notebook was generated from [_05_guards.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_05_guards.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4610b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "from typing import Optional, List, ClassVar, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import Tensor, Device, SeriesLike, ndarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36bbe25f",
   "metadata": {},
   "source": [
    "### Guards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c86e9aa",
   "metadata": {},
   "source": [
    "#### Numpy and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_matrix(arr: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `arr` is a np.matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr : SeriesLike\n",
    "        object to check whether or not it is a np.matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(arr, np.matrix)\n",
    "\n",
    "def undo_npmatrix(arr: SeriesLike) -> SeriesLike:  \n",
    "    '''\n",
    "    Given a tensor converts it to a numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor : Tensor\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    arr : ndarray\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - several graphtool functions use dependencies which rely on\n",
    "        the deprecated numpy class `np.matrix`.\n",
    "        \n",
    "    - these functions appear to be related to scipy sparse linalg\n",
    "        methods.\n",
    "    '''\n",
    "    if is_matrix(arr):\n",
    "        return np.array(arr)\n",
    "    return arr\n",
    "\n",
    "def is_series(arr: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `arr` is a pd.Series\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr : SeriesLike\n",
    "        object to check whether or not it is a pd.Series.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(arr, pd.Series)\n",
    "\n",
    "def is_series_like(series_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `series_q` is SeriesLike\n",
    "    i.e. something that is probably data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    series_q : SeriesLike\n",
    "        object to check whether or not it is a SeriesLike.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(series_q, SeriesLike)\n",
    "\n",
    "def is_np(arr_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `arr_q` is a ndarray\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr_q : SeriesLike\n",
    "        object to check whether or not it is a SeriesLike.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(arr_q, ndarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ab2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_device(device_q: Device) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `device_q` is a valid\n",
    "    pytorch device type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    device_q : Device\n",
    "        object to check whether or not it is a pytorch device.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool        \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - There is an execption. `NoneType` is a valid\n",
    "        pytorch device. Here we return `False` instead.\n",
    "    '''\n",
    "    if device_q is None:\n",
    "        return False\n",
    "    return isinstance(device_q, Device)\n",
    "\n",
    "def is_cpu(tensor: Tensor) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `tensor` is on cpu\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor : Tensor\n",
    "        object to check whether or not it is on cpu.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    # assert is_tensor(tensor)\n",
    "    if not hasattr(tensor, 'device'):\n",
    "        return True\n",
    "    return tensor.device.type == 'cpu'\n",
    "\n",
    "def is_mps(tensor: Tensor) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `tensor` is on cpu\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor : Tensor\n",
    "        object to check whether or not it is on cpu.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    # assert is_tensor(tensor)\n",
    "    if not hasattr(tensor, 'device'):\n",
    "        return False\n",
    "    return tensor.device.type == 'mps'\n",
    "\n",
    "def is_tensor(tensor_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `tensor_q` is a pytorch tensor\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor_q : Tensor\n",
    "        object to check whether or not it is a pytorch tensor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(tensor_q, Tensor)\n",
    "\n",
    "\n",
    "def is_torch(tensor_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Alias for `is_tensor`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor_q : Tensor\n",
    "        object to check whether or not it is a pytorch tensor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    is_tensor\n",
    "    '''\n",
    "    return is_tensor(tensor_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5577e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def undo_sparse(arr: SeriesLike) -> SeriesLike:  \n",
    "    '''\n",
    "    Given a arr tries to make it a dense array\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr : SeriesLike\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    arr : ndarray\n",
    "    '''\n",
    "    if hasattr(arr, 'toarray'):\n",
    "        arr = arr.toarray()\n",
    "\n",
    "    if hasattr(arr, 'todense'):\n",
    "        arr = arr.todense()\n",
    "\n",
    "    return arr\n",
    "\n",
    "def to_ndarray(arr):\n",
    "    if is_np(arr):\n",
    "        return arr\n",
    "        \n",
    "    arr = undo_npmatrix(arr)\n",
    "    \n",
    "    arr = undo_sparse(arr)\n",
    "\n",
    "    if not is_np(arr):\n",
    "        arr = np.array(arr)\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee789c1",
   "metadata": {},
   "source": [
    "## Torch utils\n",
    "> This notebook was generated from [_06_torch_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_06_torch_utils.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba329b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import Tensor, Device, SeriesLike, ndarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24fed699",
   "metadata": {},
   "source": [
    "### Torch Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b36551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    import torch\n",
    "    def ensure_device(device: Device) -> Device:\n",
    "        '''\n",
    "        Given a valid device type attempts to instantiant \n",
    "        a pytorch device object i.e. `device='cpu'` will\n",
    "        return `torch.device('cpu')`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        device : Device\n",
    "            a valid pytorch device type, possible a string.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        device : torch.device        \n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            same error if `torch.device(device)` fails\n",
    "        '''\n",
    "        if device is None:\n",
    "            return device    \n",
    "        try:\n",
    "            return torch.device(device)\n",
    "        except RuntimeError as err:\n",
    "            raise err\n",
    "        return device\n",
    "    \n",
    "    def to_cuda(tensor: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Given a tensor, ensures that it is on cuda.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        return tensor.cuda()\n",
    "\n",
    "    def to_mps(tensor: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Given a tensor, ensures that it is on mac silicon.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        return tensor.to(torch.device('mps'))\n",
    "    \n",
    "    \n",
    "    def to_torch(\n",
    "        arr: SeriesLike,\n",
    "        cuda: Optional[bool] = False,\n",
    "        mps: Optional[bool] = False,\n",
    "        device: Optional[Device] = None,\n",
    "        dtype: Optional[Any] = None\n",
    "    ) -> Tensor:\n",
    "        '''\n",
    "        Given data, ensures that it is a pytorch Tensor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        arr : SeriesLike\n",
    "        \n",
    "        cuda : bool, default=False\n",
    "            whether to return the tensor on cuda\n",
    "            \n",
    "        mps : bool, default=False\n",
    "            whether to return the tensor on mps\n",
    "            \n",
    "        device : Device, optional\n",
    "            whether to return the tensor on given device\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "            the input array as a pytorch tensor\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        - `device` takes priority over `cuda` and `mps`\n",
    "        '''\n",
    "        tensor = torch.as_tensor(arr)\n",
    "        if device is not None:\n",
    "            tensor = tensor.to(device)\n",
    "        elif cuda:\n",
    "            tensor = to_cuda(tensor)\n",
    "        elif mps:\n",
    "            tensor = to_mps(tensor)    \n",
    "        \n",
    "        if dtype is not None:\n",
    "            dtype = coerce_mps_dtype(dtype, tensor.device, assume_on_mps=False)\n",
    "            tensor = tensor.to(dtype)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    #| export\n",
    "    def to_np(tensor:Tensor) -> ndarray:\n",
    "        '''\n",
    "        Given a tensor converts it to a numpy array\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        arr : ndarray\n",
    "        '''\n",
    "        assert is_tensor(tensor)\n",
    "        if not hasattr(tensor, 'detach'):\n",
    "            try:\n",
    "                return np.array(tensor)\n",
    "            except Exception as err:\n",
    "                raise err\n",
    "        try:\n",
    "            return tensor.detach().clone().cpu().numpy()\n",
    "        except Exception as err:\n",
    "            raise err\n",
    "    \n",
    "\n",
    "    def is_mps_available() -> bool:\n",
    "        '''\n",
    "        Checks whether or not pytorch has mps availble (version) and was built with mps in mind.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : bool\n",
    "        '''\n",
    "        maybe_mps = torch.backends.mps.is_available()\n",
    "        built_mps = torch.backends.mps.is_built()\n",
    "        return maybe_mps and built_mps\n",
    "    \n",
    "    def coerce_mps_dtype(\n",
    "        dtype, \n",
    "        device: Optional[Device] = None, \n",
    "        assume_on_mps: Optional[bool] = True\n",
    "    ):\n",
    "        '''\n",
    "        Makes sure `tensor` is `torch.float32` if `tensor.dtype` is `torch.float64`\n",
    "        if `tensor.device` is `'mps'`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        dtype : any\n",
    "            dtype to check against\n",
    "        \n",
    "        device : Device, default=None\n",
    "            the device of the tensor or model from which the `dtype` comes from. If provided\n",
    "            will be used to detemine whether or not to make `torch.float64`, `torch.float32`\n",
    "            only if the device is actually `'mps'`.\n",
    "\n",
    "        assume_on_mps: bool, default=True\n",
    "            whether or not to assume that the device of choice is `'mps'`. Setting this to\n",
    "            `True` will result in `dtype` of `torch.float64` being converted to `torch.float32`\n",
    "            to try and silently fix mps errors\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dtype : any\n",
    "            the dtype, corrected for mps if needed\n",
    "        '''\n",
    "        could_be_mps = is_mps_available()\n",
    "        \n",
    "        is_float64 = dtype == torch.float64\n",
    "\n",
    "        if device is not None:\n",
    "            is_device_mps = device.type == 'mps'\n",
    "            if is_device_mps:\n",
    "                assume_on_mps = True\n",
    "\n",
    "            elif device.type == 'cuda':\n",
    "                assume_on_mps = False\n",
    "\n",
    "        \n",
    "        # NOTE: float64 not availble on mps, coerce to float32\n",
    "        # NOTE: could_be_mps and assume_on_mps both needed as\n",
    "        #       device might not be provided.\n",
    "        if could_be_mps and assume_on_mps and is_float64:\n",
    "            return torch.float32\n",
    "        \n",
    "        return dtype\n",
    "    \n",
    "    def ensure_mps_dtype(tensor: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Makes sure `tensor` is `torch.float32` if `tensor.dtype` is `torch.float64`\n",
    "        if `tensor.device` is `'mps'`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            pytorch tensor to maybe change dtype of\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        dtype = tensor.dtype\n",
    "\n",
    "        # NOTE: we don't assume mps as we explicitly pass the device\n",
    "        dtype = coerce_mps_dtype(dtype, tensor.device, assume_on_mps=False)\n",
    "\n",
    "        tensor = tensor.to(dtype)\n",
    "        return tensor\n",
    "\n",
    "    def move_to(\n",
    "        tensor: Tensor, other: Tensor, \n",
    "        dtype: Optional[Any] = None, do_dtype: Optional[bool] = True\n",
    "    ) -> Tensor:\n",
    "        '''\n",
    "        Makes sure `tensor` is on the same device as `other`\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            pytorch tensor to change device of\n",
    "            \n",
    "        other : Tensor\n",
    "            pytorch tensor we want `tensor` to be on\n",
    "            \n",
    "        dtype : optional\n",
    "            the data type to make `tensor`. If `None` will infer it\n",
    "            from `other`\n",
    "            \n",
    "        do_dype: bool, default=True\n",
    "            whether or not to just match the device of `other` or also\n",
    "            the dtype\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        \n",
    "        if not is_tensor(tensor):\n",
    "            tensor = to_torch(tensor)\n",
    "            \n",
    "        # NOTE: dtype not provided, so we will infer it\n",
    "        if dtype is None:\n",
    "            # NOTE: this little line solves mps float64 issues since \n",
    "            #       infer our tensor types and move them accordingly\n",
    "            other = ensure_mps_dtype(other)\n",
    "            dtype = other.dtype\n",
    "\n",
    "        if do_dtype:\n",
    "            tensor = tensor.to(dtype)\n",
    "\n",
    "        tensor = tensor.to(other.device)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "except ImportError as err:\n",
    "    identity = lambda x: x\n",
    "    ensure_device = identity\n",
    "    to_cuda = identity\n",
    "    to_mps = identity\n",
    "    to_torch = lambda arr, cuda, mps, device, dtype: arr\n",
    "    to_np = identity\n",
    "    is_mps_available = lambda: False\n",
    "    coerce_mps_dtype = lambda dtype, device, assume_on_mps: dtype\n",
    "    ensure_mps_dtype = identity\n",
    "    move_to = lambda tensor, other, dtype, do_dtype: tensor\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    import torch, pytorch_lightning as pl\n",
    "    def set_seeds(seed: int) -> None:\n",
    "        '''\n",
    "        Calls a bunch of seed functions with `seed`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int\n",
    "        '''    \n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)    \n",
    "        pl.seed_everything(seed)\n",
    "except ImportError as err:\n",
    "     def set_seeds(seed: int) -> None:\n",
    "         random.seed(seed)\n",
    "         np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004bad7",
   "metadata": {},
   "source": [
    "## Adata\n",
    "> This notebook was generated from [_07_adata.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_07_adata.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from typing import List, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09858a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import AnnData, ndarray, DataFrame\n",
    "from iza.static import X_MAGIC, PHATE, X_PHATE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "283075a8",
   "metadata": {},
   "source": [
    "### Adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf45ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class AdataExtractor:\n",
    "    adata: AnnData\n",
    "    layer: Optional[str] = X_MAGIC\n",
    "    x_emb: Optional[str] = X_PHATE\n",
    "\n",
    "    dim_str: Optional[str] = None\n",
    "    use_hvg: Optional[bool] = True\n",
    "\n",
    "    @property\n",
    "    def has_hvg(self):\n",
    "        return hasattr(self.adata, 'var') and hasattr(self.adata.var, 'highly_variable')\n",
    "    \n",
    "    @property\n",
    "    def has_emb(self):\n",
    "        return hasattr(self.adata, 'obsm') and self.x_emb in self.adata.obsm.keys()\n",
    "\n",
    "    def get_layer(self) -> ndarray:\n",
    "        layer = self.sdata().layers.get(self.layer, None)\n",
    "\n",
    "        if layer is None:\n",
    "            layer = self.sdata().X\n",
    "\n",
    "        if hasattr(layer, 'toarray'):\n",
    "            layer = layer.toarray()\n",
    "\n",
    "        if hasattr(layer, 'todense'):\n",
    "            layer = layer.todense()\n",
    "\n",
    "        return layer\n",
    "    \n",
    "    def get_emb(self) -> ndarray:\n",
    "        emb = self.sdata().obsm.get(self.x_emb, None)\n",
    "        if emb is None:\n",
    "            raise ValueError(f'No embedding found in adata.obsm {self.sdata().obsm.keys()}')\n",
    "\n",
    "        # NOTE: defined in _02_utils/_05_guards.ipynb\n",
    "        emb = to_ndarray(emb)\n",
    "        return emb\n",
    "\n",
    "    @property\n",
    "    def axis_str(self):\n",
    "        if self.dim_str:\n",
    "            return self.dim_str\n",
    "        return self.x_emb.replace('X_', '').upper()\n",
    "    \n",
    "    @property\n",
    "    def emb_cols(self):\n",
    "        ndim = self.get_emb().shape[1]\n",
    "        cols = [f'{self.axis_str}_{i+1}' for i in range(ndim)]\n",
    "        return cols\n",
    "        \n",
    "    def sdata(self):\n",
    "        if self.use_hvg and self.has_hvg:\n",
    "            return self.adata[:, self.adata.var.highly_variable]\n",
    "        return self.adata\n",
    "    \n",
    "    def get_df_cnt(self) -> DataFrame:\n",
    "        layer = self.get_layer()\n",
    "\n",
    "        cols = self.sdata().var.index\n",
    "        idxs = self.sdata().obs.index\n",
    "        df = pd.DataFrame(layer, index=idxs, columns=cols)\n",
    "        return df\n",
    "    \n",
    "    def get_df_emb(self) -> DataFrame:\n",
    "        emb = self.get_emb()\n",
    "        \n",
    "        cols = self.emb_cols\n",
    "        idxs = self.sdata().obs.index\n",
    "        df = pd.DataFrame(emb, index=idxs, columns=cols)\n",
    "        return df\n",
    "    \n",
    "    @property\n",
    "    def df_cnt(self):\n",
    "        return self.get_df_cnt()\n",
    "    \n",
    "    @property\n",
    "    def df_emb(self):\n",
    "        return self.get_df_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f885837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9da91c81",
   "metadata": {},
   "source": [
    "## Modules\n",
    "> This notebook was generated from [_08_modules.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_08_modules.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11055c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_rich_available() -> bool:    \n",
    "    try:\n",
    "        import rich\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_pytorch_available() -> bool:    \n",
    "    try:\n",
    "        import torch\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_anndata_available() -> bool:    \n",
    "    try:\n",
    "        import anndata \n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a7b0b",
   "metadata": {},
   "source": [
    "## Ex\n",
    "> This notebook was generated from [_09_exp.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_09_exp.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, yaml, datetime, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d00cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def config_exp_logger(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        logger\n",
    "    '''\n",
    "    basename = os.path.basename(path)\n",
    "    logger = logging.getLogger(basename)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=exp_log_filename(path), \n",
    "        encoding='utf-8',\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s\\t%(levelname)s:%(message)s',\n",
    "        datefmt='%d/%m/%Y %I:%M:%S %p',\n",
    "        filemode='w'\n",
    "    )\n",
    "    logger.info(f'Experiment path created {path}')\n",
    "    return logger\n",
    "\n",
    "def exp_log_filename(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        log_file (str): full path to provided experiment's log file\n",
    "    '''\n",
    "    return os.path.join(path, 'log.txt')\n",
    "\n",
    "def exp_param_filename(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        param_file (str): full path to provided experiment's parameter file\n",
    "    '''\n",
    "    return os.path.join(path, 'params.yml')\n",
    "\n",
    "def list_exps(path):\n",
    "    '''\n",
    "    Notes:\n",
    "    ----------\n",
    "        - an experiment is defined as a directory containing a `'params.yml'` file.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiments directory, i.e. \n",
    "            `<path-to-experiments-dir>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        experiments (str[]): experiments (subdirectories) in the specified \n",
    "            `path`.\n",
    "    '''\n",
    "    test_fn = lambda el: os.path.isdir(el) and os.path.isfile(exp_param_filename(el))\n",
    "    return list(filter(test_fn, os.listdir(path)))\n",
    "\n",
    "def gen_exp_name(name=None):\n",
    "    '''    \n",
    "    Returns:\n",
    "    ----------\n",
    "        exp_name (str): timestamp to serve as experiment name\n",
    "    '''\n",
    "    if name is None:\n",
    "        now =  datetime.datetime.now()\n",
    "        return now.strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "    return name\n",
    "        \n",
    "def load_exp_params(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        params (dict): the loaded parameters\n",
    "    '''\n",
    "    with open(exp_param_filename(path)) as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "\n",
    "def save_exp_params(path, params, logger=None):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "        params (dict): dictionary of parameters to save\n",
    "        logger (logging.Logger): Defaults to None.\n",
    "    '''\n",
    "    with open(exp_param_filename(path), 'w') as f:\n",
    "        yaml.dump(params, f, default_flow_style=False)\n",
    "    if logger: \n",
    "        logger.info('Experiment parameters saved.')\n",
    "\n",
    "def setup_exp(path, params, name=None):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to where to create experiments, i.e. \n",
    "            `<path-to-experiments-dir>`\n",
    "        params (dict): dictionary of parameters to save\n",
    "    Returns:\n",
    "    ----------\n",
    "        exp_dir (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "        logger (logging.Logger)\n",
    "    '''\n",
    "    exp_name = gen_exp_name(name)\n",
    "    exp_dir = os.path.join(path, exp_name)\n",
    "    if not os.path.isdir(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "\n",
    "    logger = config_exp_logger(exp_dir)    \n",
    "    save_exp_params(exp_dir, params, logger)\n",
    "    return exp_dir, logger\n",
    "\n",
    "    \n",
    "def is_config_subset(truth, params):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        truth (dict): dictionary of parameters to compare to\n",
    "        params (dict): dictionary of parameters to test\n",
    "    Returns:\n",
    "    ----------\n",
    "        result (bool) whether or not `params` is a subset of `truth`\n",
    "    '''\n",
    "    if not type(truth) == type(params): return False\n",
    "    for key, val in params.items():\n",
    "        if key not in truth: return False\n",
    "        if type(val) is dict:\n",
    "            if not is_config_subset(truth[key], val): \n",
    "                return False\n",
    "        else:            \n",
    "            if not truth[key] == val: return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def find_exps(path, params):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to where to create experiments, i.e. \n",
    "            `<path-to-experiments-dir>`\n",
    "        params (dict): dictionary of parameters to test\n",
    "    Returns:\n",
    "    ----------\n",
    "        results (str[]) list of experiment names where their parameters are \n",
    "            supersets of the provided `params`\n",
    "    '''\n",
    "    exps = list_exps(path)\n",
    "    results = []\n",
    "    for exp in exps:\n",
    "        exp_name = os.path.join(path, exp)\n",
    "        exp_params = load_exp_params(exp_param_filename(exp_name))\n",
    "        if is_config_subset(exp_params, params):\n",
    "            results.append(exp)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7768a5a",
   "metadata": {},
   "source": [
    "## Archive\n",
    "> This notebook was generated from [_10_archive.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_10_archive.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4570c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, pathlib, itertools\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar, Any, TypeAlias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3054dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import PathLike, PathType\n",
    "from iza.static import EXT_PY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef968730",
   "metadata": {},
   "source": [
    "### Directory Viewer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0b6e306",
   "metadata": {},
   "source": [
    "#### Archive Downloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d0a37be",
   "metadata": {},
   "source": [
    "- `Directory` defined in `_02_utils/_03_directory.ipynb`\n",
    "- `ConsoleType` defined in `_02_utils/_03_directory.ipynb`\n",
    "- `get_console` imported in `_02_utils/_03_directory.ipynb`\n",
    "- `is_rich_available` defined in `_02_utils/_08_archive.ipynb`\n",
    "- `urljoin` defined in `_02_utils/_01_files.ipynb`\n",
    "- `parse_url` imported in `_02_utils/_01_files.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ArchiveDownloader:    \n",
    "    _: KW_ONLY\n",
    "    rootdir: str \n",
    "    archive: str\n",
    "    entries: Union[str, list[str]]\n",
    "    savedir: str\n",
    "    extract: bool = False\n",
    "    cleanup: bool = False\n",
    "    compound_archive: bool = False\n",
    "    archives: Optional[list[str]] = None\n",
    "    console: Optional[ConsoleType] = None\n",
    "    progress: Optional[ProgressType] = None\n",
    "\n",
    "    \n",
    "\n",
    "    def __post_init__(self):        \n",
    "        self.entries = self.entries if isinstance(self.entries, list) else [self.entries]\n",
    "        if is_rich_available():\n",
    "            self.console = get_console()\n",
    "            self.progress = self.get_progress()\n",
    "\n",
    "        self.savedir = Path(self.savedir).expanduser()\n",
    "        make_missing_dirs(self.savedir)\n",
    "\n",
    "    def get_progress(self):\n",
    "        if is_rich_available():\n",
    "            progress = getattr(self, 'progress', None)\n",
    "            if progress is None and Progress is not None:\n",
    "                self.progress = Progress(console=self.console)\n",
    "                return self.progress\n",
    "\n",
    "        elif Progress is None:\n",
    "            return None\n",
    "        \n",
    "        elif Progress is not None:\n",
    "            self.progress = Progress(console=self.console)\n",
    "            return self.progress\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        return urljoin(self.rootdir, self.archive)\n",
    "\n",
    "    @property\n",
    "    def urls(self) -> list[str]:\n",
    "        urls = []\n",
    "        if self.compound_archive and self.archives is not None:\n",
    "            for archive, entry in itertools.product(self.archives, self.entries):\n",
    "                urls.append(urljoin(self.rootdir, archive, entry))\n",
    "        else:\n",
    "            urls = [urljoin(self.path, entry) for entry in self.entries]\n",
    "        return urls\n",
    "\n",
    "    def download_missing_files(self) -> None:\n",
    "        total_files = len(self.urls)\n",
    "        if is_rich_available() and self.progress is not None:\n",
    "            with self.progress:\n",
    "                task = self.progress.add_task(\"[cyan]Downloading...\", total=total_files)\n",
    "                for url in self.urls:\n",
    "                    filename = Path(parse_url(url).path).name\n",
    "                    fullpath = self.savedir / filename\n",
    "                    if not fullpath.exists():\n",
    "                        stream_file(url, str(fullpath))\n",
    "                        self.progress.advance(task)\n",
    "        else:            \n",
    "            for url in tqdm(self.urls, desc='Downloading'):       \n",
    "                filename = Path(parse_url(url).path).name\n",
    "                fullpath = self.savedir / filename\n",
    "                if not fullpath.exists():\n",
    "                    stream_file(url, str(fullpath))\n",
    "                # print(\".\", end=\"\")\n",
    "\n",
    "    def extract_files(self) -> None:\n",
    "        files = [self.savedir / entry for entry in self.entries]\n",
    "        if is_rich_available() and self.progress is not None:\n",
    "            with self.progress:\n",
    "                task = self.progress.add_task(\"[cyan]Extracting...\", total=len(files))\n",
    "                for file in files:\n",
    "                    if is_tarball(file):\n",
    "                        decompress_tarball(file)\n",
    "                    elif is_gz(file):\n",
    "                        decompress_gunzip(file, remove=self.cleanup)\n",
    "                    self.progress.advance(task)\n",
    "        else:\n",
    "            for file in tqdm(files, desc='Extracting'):\n",
    "                if is_tarball(file):\n",
    "                    decompress_tarball(file)\n",
    "                elif is_gz(file):\n",
    "                    decompress_gunzip(file, remove=self.cleanup)\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        if is_rich_available():\n",
    "            self.console.print(f\"Processing archive: [bold cyan]{self.archive}[/bold cyan]\")\n",
    "        else:\n",
    "            print(f\"Processing archive: {self.archive}\")\n",
    "        self.download_missing_files()\n",
    "        if self.extract:\n",
    "            self.extract_files()\n",
    "\n",
    "        dir = Directory(self.savedir)\n",
    "        if is_rich_available():\n",
    "            dir.print_rich(self.console)\n",
    "        else:\n",
    "            dir.print()\n",
    "\n",
    "@dataclass\n",
    "class AmazonArchiveDownloader(ArchiveDownloader):\n",
    "    bucket: str\n",
    "    region: str = 'us-east-2'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.rootdir = f\"https://{self.bucket}.s3.{self.region}.amazonaws.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
