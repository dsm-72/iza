{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bce3f47",
   "metadata": {},
   "source": [
    "# utils\n",
    "> This notebook was generated from the following notebooks:\n",
    "\n",
    "- [_00_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_00_utils.ipynb)\n",
    "- [_01_files.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_01_files.ipynb)\n",
    "- [_02_slice.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_02_slice.ipynb)\n",
    "- [_03_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_03_directory.ipynb)\n",
    "- [_04_filter_matrix_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_04_filter_matrix_directory.ipynb)\n",
    "- [_05_guards.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_05_guards.ipynb)\n",
    "- [_06_torch_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_06_torch_utils.ipynb)\n",
    "- [_07_adata.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_07_adata.ipynb)\n",
    "- [_09_exp.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_09_exp.ipynb)\n",
    "- [_10_archive.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_10_archive.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb59a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e2995",
   "metadata": {},
   "source": [
    "## Utils\n",
    "> This notebook was generated from [_00_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_00_utils.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb70e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect, string\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from typing import List, Any, Optional, Callable, Union, Tuple, Iterable, Set, TypeAlias, Type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5739c242",
   "metadata": {},
   "source": [
    "### Basic Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def isiter(val: Any) -> bool:    \n",
    "    return isinstance(val, Iterable)\n",
    "\n",
    "def allinstance(vals:Any, dtype:Union[Type, TypeAlias]=Any) -> bool:\n",
    "    return isiter(vals) and all(isinstance(i, dtype) for i in vals)\n",
    "\n",
    "def allsametype(vals:Any) -> bool:\n",
    "    if not isiter(vals) or len(vals) == 0: return True\n",
    "    dtype = type(vals[0])\n",
    "    return isiter(vals) and all(isinstance(i, dtype) for i in vals)\n",
    "\n",
    "def isin(val:Any, vals:Iterable) -> bool:\n",
    "    return val in vals\n",
    "\n",
    "def arein(vals:Iterable, refs:Iterable) -> bool:                                             \n",
    "    return isiter(vals) and isiter(refs) and all(isin(v, refs) for v in vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636138ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def str_just_alpha(s:str) -> str:\n",
    "    '''Filters a string for just alpha values'''\n",
    "    return ''.join(list(filter(str.isalpha, s)))\n",
    "\n",
    "def str_just_numeric(s:str) -> str:\n",
    "    '''Filters a string for just numeric values'''\n",
    "    return ''.join(list(filter(str.isnumeric, s)))\n",
    "\n",
    "def strip_punc(s:str) -> str:\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef1eec0a",
   "metadata": {},
   "source": [
    "### Argument and Key-Word Argument Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe839eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_kwargs_for_func(fn: Callable, **kwargs:Optional[dict]):\n",
    "    params = inspect.signature(fn).parameters\n",
    "    return {k:v for k,v in kwargs.items() if k in params}\n",
    "\n",
    "def filter_kwargs_for_class(cls: Callable, **kwargs:Optional[dict]):\n",
    "    params = inspect.signature(cls.__init__).parameters\n",
    "    return {k:v for k,v in kwargs.items() if k in params}\n",
    "\n",
    "def wrangle_kwargs_for_func(\n",
    "    fn: Callable, \n",
    "    defaults: Optional[dict]=None,\n",
    "    **kwargs:Optional[dict]\n",
    ") -> dict:\n",
    "    # copy defaults\n",
    "    params = (defaults or {}).copy()\n",
    "    # update with kwargs of our function\n",
    "    params.update(kwargs or {})\n",
    "    # filter for only the params that other function accepts\n",
    "    params = filter_kwargs_for_func(fn, **params)\n",
    "    return params\n",
    "\n",
    "def wrangle_kwargs_for_class(\n",
    "    cls: Callable, \n",
    "    defaults: Optional[dict]=None,\n",
    "    **kwargs:Optional[dict]\n",
    ") -> dict:\n",
    "    # copy defaults\n",
    "    params = (defaults or {}).copy()\n",
    "    # update with kwargs of our class\n",
    "    params.update(kwargs or {})\n",
    "    # filter for only the params that other class accepts\n",
    "    params = filter_kwargs_for_class(cls, **params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3a286",
   "metadata": {},
   "source": [
    "## Files\n",
    "> This notebook was generated from [_01_files.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_01_files.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cf64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, sys, pwd, atexit, tempfile, inspect, pathlib\n",
    "import requests, tarfile, gzip, shutil\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Optional, List, Union, Iterable, Tuple, Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a26122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.static import (\n",
    "    EXT_GZ, EXT_TAR, EXT_TAR_GZ,\n",
    ")\n",
    "\n",
    "from iza.types import (\n",
    "    PathType, PathLike\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e94cd125",
   "metadata": {},
   "source": [
    "### PathLib Interoperabliilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_pathlib(path:Any) -> bool:\n",
    "    \"\"\"Check if the input is a path\"\"\"\n",
    "    return isinstance(path, PathType)\n",
    "\n",
    "def is_pathlike(path:Any) -> bool:\n",
    "    \"\"\"Check if the input is a path\"\"\"\n",
    "    return isinstance(path, (PathLike))\n",
    "\n",
    "def is_path(path:Any, existance:Optional[bool]) -> bool:\n",
    "    \"\"\"Check if the input is a path\"\"\"\n",
    "    exists_q = os.path.exists(os.path.expanduser(path))\n",
    "    return is_pathlike(path) and (exists_q if existance else True)\n",
    "\n",
    "def str_to_path(path:str) -> PathType:\n",
    "    \"\"\"Convert a string to a pathlib.Path object\"\"\"\n",
    "    return pathlib.Path(path)\n",
    "\n",
    "def path_to_str(path: PathType) -> str:\n",
    "    \"\"\"Convert a pathlib.Path object to a string\"\"\"\n",
    "    return os.fspath(path)\n",
    "\n",
    "def to_abs_expanded(dirname:Optional[PathLike]=None) -> PathType:\n",
    "    if dirname is None:\n",
    "        dirname = os.getcwd()\n",
    "\n",
    "    dirname = os.path.expanduser(dirname) \n",
    "    dirname = os.path.abspath(dirname) \n",
    "\n",
    "    if not is_pathlib(dirname):\n",
    "        dirname = pathlib.Path(dirname)\n",
    "\n",
    "    return dirname\n",
    "\n",
    "def sort_file_first(path: PathType):\n",
    "    return (not path.is_file(), path.name.lower())\n",
    "\n",
    "def sort_directory_first(path: PathType):\n",
    "    return (path.is_file(), path.name.lower())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03903ff9",
   "metadata": {},
   "source": [
    "### User Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_user() -> str:\n",
    "    user = pwd.getpwuid(os.getuid())[0]\n",
    "    return user\n",
    "\n",
    "def collapse_user(path: str) -> str:\n",
    "    _, rest = path.split(get_user())    \n",
    "    return '~' + rest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0477ba72",
   "metadata": {},
   "source": [
    "### Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a460ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_ext(filename:str, extension:str) -> bool:\n",
    "    if is_pathlib(filename):\n",
    "        filename = path_to_str(filename)\n",
    "    has_extension = extension in filename \n",
    "    splits = filename.split(extension)\n",
    "    is_end_of_str = len(splits) >= 2 and splits[-1] == ''\n",
    "    is_end_of_str = filename.endswith(extension)\n",
    "    return has_extension and is_end_of_str\n",
    "\n",
    "def drop_ext(filename:str, extension:Optional[str]=None) -> str:\n",
    "    if is_pathlib(filename):\n",
    "        filename = path_to_str(filename)\n",
    "        \n",
    "    file = os.path.basename(filename)\n",
    "    if extension is None:\n",
    "        file, *_ = file.split('.')\n",
    "    else:\n",
    "        file = filename.replace(extension, '')\n",
    "    return os.path.join(os.path.dirname(filename), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_tar(filename:str) -> bool:\n",
    "    return check_ext(filename, EXT_TAR)\n",
    "\n",
    "def is_gz(filename:str) -> bool:\n",
    "    return check_ext(filename, EXT_GZ)\n",
    "\n",
    "def is_targz(filename:str) -> bool:\n",
    "    return check_ext(filename, EXT_TAR_GZ)\n",
    "\n",
    "def is_tarball(filename:str) -> bool:\n",
    "    return is_tar(filename) or is_targz(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe937eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_for_gz_files(files:List[str]) -> List[str]:\n",
    "    return list(filter(lambda f: is_gz(f), files))\n",
    "\n",
    "def get_gz_files_in_dir(dirname:str) -> List[str]:\n",
    "    all_files = []\n",
    "\n",
    "    for (root, dirs, files) in os.walk(dirname):   \n",
    "        fullpaths = [os.path.join(root, file) for file in files]\n",
    "        all_files.extend(fullpaths)\n",
    "    \n",
    "    gz_files = filter_for_gz_files(all_files)\n",
    "    return gz_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06dd0e51",
   "metadata": {},
   "source": [
    "### Decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decompress_tarball(filename:str, remove:bool=False) -> Tuple[str, Optional[EOFError]]:\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "        dirname : str\n",
    "            The name of the archive e.g. `~/Downloads/fluentbio.tar.gz` would\n",
    "            yield `~/Downloads/fluentbio`\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    FluentBio has a weird gzip so it complains when it is \n",
    "        actually fine\n",
    "    '''\n",
    "    error = None\n",
    "    decompress_dir = os.path.dirname(filename)\n",
    "    ext = EXT_TAR_GZ if is_targz(filename) else EXT_TAR if is_tar(filename) else EXT_GZ\n",
    "\n",
    "    dirname = drop_ext(filename, ext)\n",
    "    try:\n",
    "        with tarfile.open(filename) as tarball:\n",
    "            tarball.extractall(decompress_dir)\n",
    "            tarball.close()\n",
    "\n",
    "    except EOFError as error:\n",
    "        pass\n",
    "    \n",
    "    if os.path.isdir(dirname) and remove:\n",
    "        os.remove(filename)\n",
    "\n",
    "    return dirname, error\n",
    "\n",
    "\n",
    "def decompress_gunzip(filename:str, remove:bool=False) -> Tuple[str, Optional[EOFError]]:\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "        file : str\n",
    "            The name of the decompressed file e.g. `~/Downloads/fluentbio.tsv.gz` would\n",
    "            yield `~/Downloads/fluentbio.tsv`\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    FluentBio has a weird gzip so it complains when it is \n",
    "        actually fine\n",
    "    '''\n",
    "    error = None\n",
    "    decompressed_file = drop_ext(filename, EXT_GZ)\n",
    "    try:             \n",
    "        with gzip.open(filename, 'rb') as gunzipped:\n",
    "            with open(decompressed_file, 'wb') as unzipped:\n",
    "                shutil.copyfileobj(gunzipped, unzipped)     \n",
    "                   \n",
    "    except EOFError as error:\n",
    "        pass\n",
    "\n",
    "    if os.path.isfile(decompressed_file) and remove:\n",
    "        os.remove(filename)\n",
    "\n",
    "    return decompressed_file, error\n",
    "\n",
    "def undo_gz(filename: str, remove:bool=False) -> str:\n",
    "    if is_targz(filename):\n",
    "        filename, _ = decompress_tarball(filename, remove)\n",
    "    elif is_gz(filename):\n",
    "        filename, _ = decompress_gunzip(filename, remove)\n",
    "    elif is_tar(filename):\n",
    "        pass\n",
    "    elif is_tarball(filename):\n",
    "        filename, _ = decompress_tarball(filename, remove)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f40f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DecompressFunc = Callable[[str, bool], str]\n",
    "\n",
    "@dataclass\n",
    "class RecursiveDecompressor:\n",
    "    dirname: PathType\n",
    "    entries: Optional[List[str]] = field(default_factory=list, repr=False)\n",
    "    strategy: Optional[DecompressFunc] = field(default=undo_gz, repr=False)\n",
    "    remove: Optional[bool] = field(default=True, repr=False)\n",
    "    progress: Optional[Any] = field(default=None, repr=False, init=True)\n",
    "\n",
    "    total: Optional[int] = field(default=None, repr=False, init=False)\n",
    "    done: Optional[int] = field(default=None, repr=False, init=False)\n",
    "    task: Optional[Any] = field(default=None, repr=False, init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not is_pathlib(self.dirname): \n",
    "            self.dirname = pathlib.Path(self.dirname)\n",
    "            \n",
    "        if self.entries is None: \n",
    "            self.entries = os.listdir(self.dirname)\n",
    "\n",
    "        self.done = 0\n",
    "        self.total = len(self.entries)\n",
    "\n",
    "    def execute(self):\n",
    "        return self.decompress()\n",
    "\n",
    "    def update_total(self, increment: int):\n",
    "        self.total += increment\n",
    "        if self.progress is not None:\n",
    "            self.progress.update(self.task, total=self.total)\n",
    "        else:\n",
    "            self.tqdm_bar.total = self.total\n",
    "            self.tqdm_bar.refresh()\n",
    "\n",
    "    def decrease_total(self):\n",
    "        self.total -= 1\n",
    "        if self.progress is not None:\n",
    "            self.progress.update(self.task, total=self.total)\n",
    "        else:\n",
    "            self.tqdm_bar.total = self.total\n",
    "            self.tqdm_bar.refresh()\n",
    "\n",
    "    def decompress(self):\n",
    "        files = [self.dirname / entry for entry in self.entries]\n",
    "\n",
    "        if self.progress is not None:\n",
    "            with self.progress:\n",
    "                self.task = self.progress.add_task(\"[cyan]Extracting...\", total=self.total)\n",
    "                for file in files:\n",
    "                    self.recurse(file)\n",
    "                    self.progress.advance(self.task)\n",
    "        else:\n",
    "            with tqdm(total=self.total, desc='Extracting') as self.tqdm_bar:\n",
    "                for file in files:\n",
    "                    self.recurse(file)\n",
    "                    self.tqdm_bar.update()\n",
    "\n",
    "    def recurse(self, current:PathLike):\n",
    "        filename = path_to_str(current)\n",
    "        nextfile = self.strategy(filename, self.remove)\n",
    "        self.done += 1\n",
    "        nextfile = pathlib.Path(nextfile)\n",
    "        \n",
    "        # If the decompressed file is a directory, recursively decompress it\n",
    "        if nextfile.is_dir():\n",
    "            for root, dirs, files in os.walk(nextfile):\n",
    "                for f in files:\n",
    "                    self.update_total(1)\n",
    "                    self.recurse(pathlib.Path(root) / f)\n",
    "                    \n",
    "        elif self.remove:\n",
    "            if filename in self.entries:\n",
    "                self.entries.remove(filename)\n",
    "            self.decrease_total()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65c74365",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54da508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_missing_dirs(dirs:List[str]):\n",
    "    if isinstance(dirs, str):\n",
    "        dirs = [dirs]\n",
    "        \n",
    "    elif is_pathlib(dirs):\n",
    "        dirs = [dirs]\n",
    "        \n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "            \n",
    "def dir_dirs(dirname:str) -> List[str]:\n",
    "    entries = os.listdir(dirname)\n",
    "    is_subdir = lambda e : os.path.isdir(os.path.join(dirname, e))\n",
    "    return list(filter(is_subdir, entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad8ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decompress_directory_of_gunzipped_files(\n",
    "    dirname:str, desc:Optional[str]=None, remove:Optional[bool]=False\n",
    ") -> None:\n",
    "    if desc is None:\n",
    "        desc = dirname.split('/')[-1]\n",
    "\n",
    "    gz_files = get_gz_files_in_dir(dirname)\n",
    "    for filename in tqdm(gz_files, desc=desc):\n",
    "        decomp_filename, error = decompress_gunzip(filename, remove)   \n",
    "\n",
    "\n",
    "def decompress_tarball_of_gunzipped_files(\n",
    "    filename:str, desc:Optional[str]=None, remove:Optional[bool]=False\n",
    ") -> None:\n",
    "    # NOTE: initial decompress of .tar.gz\n",
    "    dirname, error = decompress_tarball(filename)\n",
    "\n",
    "    if desc is None:\n",
    "        desc = dirname.split('/')[-1]\n",
    "\n",
    "    # NOTE: decompress all internal .gz files\n",
    "    decompress_directory_of_gunzipped_files(dirname, desc, remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "943c3c7f",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_file(uri:str, filename:Optional[str]=None, desc:Optional[str]=None) -> None:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    uri : str\n",
    "        The URI to download\n",
    "\n",
    "    filename : str, optional\n",
    "        The fullpath name of the file to download. Defaults to \n",
    "        `~/Downloads/os.path.basename(uri)`.\n",
    "\n",
    "    desc : str, optional\n",
    "        The description of the `tqdm` progress bar. Defaults to \n",
    "        `os.path.basename(uri)`.\n",
    "    '''\n",
    "    if filename is None:\n",
    "        download_dir = os.path.expanduser(f'~/Downloads')        \n",
    "        filename = os.path.join(download_dir, os.path.basename(uri))\n",
    "\n",
    "    basename = os.path.basename(filename)\n",
    "    if desc is None:\n",
    "        desc = basename\n",
    "\n",
    "    response = requests.get(uri, stream=True)\n",
    "    total = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with tqdm.wrapattr(\n",
    "        open(filename, 'wb'), 'write', \n",
    "        miniters=1, desc=desc, total=total\n",
    "    ) as fout:\n",
    "        for chunk in response.iter_content(chunk_size=4096):\n",
    "            fout.write(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efc2793f",
   "metadata": {},
   "source": [
    "### Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_and_decompress_tarball_of_gunzipped_files(\n",
    "    uri:str, download_dir:str=None, desc:Optional[str]=None, remove:Optional[bool]=False\n",
    "):\n",
    "    filename = os.path.basename(uri)\n",
    "    fullpath = os.path.join(download_dir, filename)\n",
    "\n",
    "    if desc is None:\n",
    "        description = f'Downloading {filename}'\n",
    "\n",
    "\n",
    "    # NOTE: Amazon --> filtered_matrix.tar.gz\n",
    "    stream_file(uri, description)\n",
    "\n",
    "    if desc is None:\n",
    "        description = f'Decompressing {filename}'\n",
    "    # NOTE: filtered_matrix.tar.gz --> filtered_matrix/**/file.tsv\n",
    "    decompress_tarball_of_gunzipped_files(fullpath, desc, remove)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80b6db5d",
   "metadata": {},
   "source": [
    "### Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676878bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_temp_file(**kwargs: Any) -> tempfile.NamedTemporaryFile:\n",
    "    temp = tempfile.NamedTemporaryFile(**kwargs)\n",
    "    @atexit.register\n",
    "    def delete_temp() -> None:\n",
    "        temp.close()\n",
    "    return temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02fd6318",
   "metadata": {},
   "source": [
    "### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602562b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from urllib3.util.url import parse_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def urljoin(*parts: str) -> str:\n",
    "    return parse_url('/'.join(s.strip('/') for s in parts)).url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b21d9",
   "metadata": {},
   "source": [
    "## Slice\n",
    "> This notebook was generated from [_02_slice.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_02_slice.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711052d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "105d6914",
   "metadata": {},
   "source": [
    "### Slice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79a0a869",
   "metadata": {},
   "source": [
    "Helps convert `slices` to its numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784cb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Slice:\n",
    "    \"\"\"A class for representing a slice and providing conversion to other formats.\"\"\"\n",
    "    slc: slice = field(default_factory=slice)\n",
    "\n",
    "    @property\n",
    "    def start(self):\n",
    "        try:\n",
    "            return self._start\n",
    "        except AttributeError:\n",
    "            self._start = self.slc.start\n",
    "        return self._start\n",
    "    \n",
    "    @start.setter\n",
    "    def start(self, value):\n",
    "        \"\"\"Sets the start index.\"\"\"\n",
    "        if value < 0:\n",
    "            raise ValueError(\"Slice indices must be non-negative.\")\n",
    "        self._start = value\n",
    "\n",
    "    @property\n",
    "    def stop(self):\n",
    "        try:\n",
    "            return self._stop\n",
    "        except AttributeError:\n",
    "            self._stop = self.slc.stop\n",
    "        return self._stop\n",
    "    \n",
    "    @stop.setter\n",
    "    def stop(self, value):\n",
    "        \"\"\"Sets the stop index.\"\"\"\n",
    "        if value < 0:\n",
    "            raise ValueError(\"Slice indices must be non-negative.\")\n",
    "        self._stop = value\n",
    "    \n",
    "    @property\n",
    "    def step(self):\n",
    "        \"\"\"Gets the step index.\"\"\"\n",
    "        try:\n",
    "            return self._step\n",
    "        except AttributeError:\n",
    "            self._step = self.slc.step\n",
    "        return self._step\n",
    "    \n",
    "    @step.setter\n",
    "    def step(self, value):\n",
    "        \"\"\"Sets the step index.\"\"\"\n",
    "        if value < 0:\n",
    "            raise ValueError(\"Slice step must be non-negative.\")\n",
    "        self._step = value\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.start is None:\n",
    "            self.start = 0\n",
    "\n",
    "        if self.stop is None:\n",
    "            self.stop = min(0, self.start, max(1, self.start))\n",
    "\n",
    "        if self.step is None:\n",
    "            self.step = 1\n",
    "\n",
    "    def totuple(self) -> Tuple[Union[int, float, None], Union[int, float, None], Union[int, float, None]]:\n",
    "        \"\"\"Converts the slice to a tuple.\"\"\"\n",
    "        return (self.start, self.stop, self.step)\n",
    "    \n",
    "    def toslice(self) -> slice:\n",
    "        \"\"\"Converts the updated slice.\"\"\"\n",
    "        return slice(self.start, self.stop, self.step)\n",
    "    \n",
    "    def tolist(self) -> List[Union[int, float]]:\n",
    "        \"\"\"Converts the slice to a list.\"\"\"\n",
    "        return list(range(self.start, self.stop, self.step))\n",
    "    \n",
    "    def todict(self) -> List[Union[int, float]]:\n",
    "        \"\"\"Converts the slice to a dict.\"\"\"\n",
    "        return dict(zip('start stop step'.split(), self.totuple()))\n",
    "        \n",
    "\n",
    "    def astype(self, dtype:str):\n",
    "        \"\"\"Converts the slice to a specified format.\"\"\"\n",
    "        if dtype in {'list', list}:\n",
    "            return self.tolist()\n",
    "        elif dtype in {'numpy', np.ndarray}:\n",
    "            return np.array(self.tolist())\n",
    "        elif dtype in {'pandas', pd.Series}:\n",
    "            return pd.Series(self.tolist())\n",
    "        elif dtype in {'tuple', tuple}:\n",
    "            return self.totuple()\n",
    "        elif dtype in {'dict', dict}:\n",
    "            return self.todict()\n",
    "        elif dtype in {'slice', slice}:\n",
    "            return self.toslice()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b12c5",
   "metadata": {},
   "source": [
    "## Director\n",
    "> This notebook was generated from [_03_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_03_directory.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe909aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, pathlib \n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar, Any, TypeAlias, Callable\n",
    "from enum import StrEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ffa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import (\n",
    "    PathLike, PathType,\n",
    "    DirectoryTreeStrings, TreeEntryFunc,\n",
    "    RichTree, RichText, RichConsole, RichProgress\n",
    ")\n",
    "from iza.static import EXT_PY\n",
    "\n",
    "#| export\n",
    "from iza.imp import RichImp\n",
    "from ipos.imp import is_mod, is_var_imp, Module, Imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:    \n",
    "    from rich.tree import Tree\n",
    "    from rich.text import Text    \n",
    "    from rich.filesize import decimal\n",
    "    from rich import get_console\n",
    "    from rich.console import Console\n",
    "    from rich.progress import Progress\n",
    "\n",
    "except ImportError:\n",
    "    Tree = None\n",
    "    Text = None \n",
    "    Progress = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a04e22b",
   "metadata": {},
   "source": [
    "### Directory Viewer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea8e3db1",
   "metadata": {},
   "source": [
    "#### Default Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd14ee4e",
   "metadata": {},
   "source": [
    "##### Init Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca63e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def base_init_tree(dirname: str):\n",
    "    return None\n",
    "\n",
    "def rich_init_tree(dirname):\n",
    "    return Tree(f'[link file://{dirname}]{dirname}', guide_style='bold bright_blue')\n",
    "\n",
    "def init_tree(dirname):\n",
    "    try:\n",
    "        return rich_init_tree(dirname)\n",
    "    except:\n",
    "        return base_init_tree(dirname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32847347",
   "metadata": {},
   "source": [
    "##### Walked Entry to Str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def base_entry_fn(path:Path, prefix:str='', pointer:str='', suffix:str='') -> str:\n",
    "    name = path.name\n",
    "    return f'{prefix}{pointer}{name}{suffix}'\n",
    "\n",
    "def rich_entry_fn(path:Path, prefix:str='', pointer:str='', suffix:str='') -> str:\n",
    "    text = Text(path.name)\n",
    "    size = decimal(path.stat().st_size)\n",
    "    text.stylize(f'link file://{path}')\n",
    "    text.append(f' ({size})', 'cyan')\n",
    "    return text\n",
    "\n",
    "def entry_fn(path:Path, prefix:str='', pointer:str='', suffix:str='') -> str:\n",
    "    try:\n",
    "        return rich_entry_fn(path, prefix, pointer, suffix)\n",
    "    except:\n",
    "        return base_entry_fn(path, prefix, pointer, suffix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2477b41",
   "metadata": {},
   "source": [
    "#### Walk Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b583341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def walk_dir_tree(\n",
    "    dirname: PathLike, \n",
    "    prefix: str = '',\n",
    "    hidden: Optional[bool] = False, \n",
    "    tree: Optional[RichTree] = None,\n",
    "    entry_fn: TreeEntryFunc = entry_fn,\n",
    "):    \n",
    "    '''\n",
    "    A recursive generator, given a directory Path object\n",
    "    will yield a visual tree structure line by line\n",
    "    with each line prefixed by the same characters\n",
    "    Notes\n",
    "    -----\n",
    "    Adapted from https://stackoverflow.com/a/59109706/5623899\n",
    "    '''\n",
    "    # NOTE: sort_directory_first, to_abs_expanded, rich_file defined in _02_utils/_08_modules.ipynb\n",
    "    dirname = Path(to_abs_expanded(dirname))\n",
    "    contents = sorted(Path(dirname).iterdir(), key=sort_directory_first)\n",
    "    \n",
    "    SPACE, BRANCH, TEE, LAST = DirectoryTreeStrings\n",
    "    pointers = [TEE] * (len(contents) - 1) + [LAST]\n",
    "    \n",
    "    for pointer, path in zip(pointers, contents):\n",
    "        # Remove hidden files\n",
    "        if path.name.startswith('.') and not hidden:\n",
    "            continue\n",
    "        \n",
    "        name = entry_fn(path, prefix, pointer, suffix='')\n",
    "        yield name\n",
    "        \n",
    "        branch = None\n",
    "        if tree is not None:\n",
    "            branch = tree.add(entry_fn(path))\n",
    "    \n",
    "        if path.is_dir():\n",
    "            # NOTE: space because last, └── , above so no more |\n",
    "            extension = BRANCH if pointer == TEE else SPACE\n",
    "            yield from walk_dir_tree(path, prefix=f'{prefix}{extension}', hidden=hidden, tree=branch, entry_fn=entry_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac3ed6b5",
   "metadata": {},
   "source": [
    "#### DirectoryTree\n",
    "walks through directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DirectoryTree:\n",
    "    dirname: str\n",
    "    \n",
    "    hidden: Optional[bool] = False\n",
    "    entry_fn: Optional[TreeEntryFunc] = field(default=entry_fn)\n",
    "    tree: Optional[RichTree] = field(init=False, default=None)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # NOTE: defined in _02_utils/_01_files.ipynb\n",
    "        self.dirname = Path(to_abs_expanded(self.dirname))\n",
    "        if self.tree is None:\n",
    "            self.tree = init_tree(self.dirname)\n",
    "\n",
    "    def tree_generator(self):\n",
    "        yield from walk_dir_tree(\n",
    "            self.dirname, hidden=self.hidden, \n",
    "            tree=self.tree, entry_fn=self.entry_fn\n",
    "        )\n",
    "        \n",
    "    def get_tree_lines(self) -> List[str]:        \n",
    "        tree_gen = self.tree_generator()\n",
    "        lines = [line for line in tree_gen]\n",
    "        return lines\n",
    "    \n",
    "    def make_tree_str(self) -> str:        \n",
    "        lines = self.get_tree_lines()\n",
    "        tree_str = '\\n'.join([str(self.dirname), *lines])\n",
    "        return tree_str\n",
    "   \n",
    "    def print(self) -> None:        \n",
    "        tree_str = self.make_tree_str()        \n",
    "        print(tree_str)\n",
    "        return\n",
    "\n",
    "    def __repr__(self):        \n",
    "        tree_str = self.make_tree_str()        \n",
    "        return tree_str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb234cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@dataclass\n",
    "class RichDirectory(DirectoryTree):\n",
    "    _: KW_ONLY\n",
    "    console: Optional[RichConsole] = field(default_factory=Console, init=True, repr=False)    \n",
    "    _imp: Imp = field(default_factory=RichImp, init=False, repr=False)\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self._imp = RichImp()\n",
    "    \n",
    "        if self.console is None and is_mod(self._imp._module):\n",
    "            self.console = get_console()\n",
    "\n",
    "    def print_rich(self) -> None:    \n",
    "        lines = self.get_tree_lines()\n",
    "        self.console.print(self.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@dataclass\n",
    "class Directory(DirectoryTree):\n",
    "    dirname: PathLike\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532ad2d",
   "metadata": {},
   "source": [
    "## Filter matrix director\n",
    "> This notebook was generated from [_04_filter_matrix_directory.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_04_filter_matrix_directory.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be539023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99013b14",
   "metadata": {},
   "source": [
    "### Filter Matrix Directory Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa822782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.static import (\n",
    "    ADATA, MATRIX, BARCODES, FEATURES, EXT_H5, EXT_MTX, EXT_TSV,\n",
    "    GENE_SYMBOL, ENSEMBL_ID\n",
    ")\n",
    "from iza.types import (\n",
    "    AnnData\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try: \n",
    "    import scanpy as sc, scprep\n",
    "\n",
    "    # NOTE: Directory defined in _02_utils/_02_directory.ipynb\n",
    "    @dataclass\n",
    "    class FilterMatrixDirectory(Directory):\n",
    "        _: KW_ONLY\n",
    "        ADATA_FILE: ClassVar[str] = f'{ADATA}{EXT_H5}'\n",
    "        MATRIX_FILE: ClassVar[str] = f'{MATRIX}{EXT_MTX}'\n",
    "        BARCODES_FILE: ClassVar[str] = f'{BARCODES}{EXT_TSV}'\n",
    "        FEATURES_FILE: ClassVar[str] = f'{FEATURES}{EXT_TSV}'\n",
    "        \n",
    "\n",
    "        def __post_init__(self):    \n",
    "            try:\n",
    "                if not self.has_adata:\n",
    "                    self.make_adata()\n",
    "            except Exception as err:\n",
    "                raise err\n",
    "\n",
    "        def __repr__(self):\n",
    "            base = os.path.basename(self.dirname)\n",
    "            srep = f'FilteredMatrix(valid: {self.is_valid}, adata: {self.has_adata})'        \n",
    "            srep += '\\n'\n",
    "            srep += super(FilterMatrixDirectory, self).__repr__()\n",
    "            return srep\n",
    "                    \n",
    "        @property\n",
    "        def adata_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.ADATA_FILE)\n",
    "\n",
    "        @property\n",
    "        def matrix_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.MATRIX_FILE)\n",
    "        \n",
    "        @property\n",
    "        def barcodes_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.BARCODES_FILE)\n",
    "        \n",
    "        @property\n",
    "        def features_filename(self) -> str:\n",
    "            return os.path.join(self.dirname, self.FEATURES_FILE)\n",
    "\n",
    "        @property\n",
    "        def has_adata(self) -> bool:\n",
    "            return os.path.isfile(self.adata_filename)\n",
    "\n",
    "        @property\n",
    "        def has_matrix(self) -> bool:\n",
    "            return os.path.isfile(self.matrix_filename)\n",
    "\n",
    "        @property\n",
    "        def has_barcodes(self) -> bool:\n",
    "            return os.path.isfile(self.barcodes_filename)\n",
    "\n",
    "        @property\n",
    "        def has_features(self) -> bool:\n",
    "            return os.path.isfile(self.features_filename)\n",
    "\n",
    "        @property\n",
    "        def is_valid(self) -> bool:\n",
    "            return all([self.has_matrix, self.has_barcodes, self.has_features])\n",
    "\n",
    "        def make_adata(self) -> AnnData:\n",
    "            if self.has_adata:\n",
    "                return\n",
    "\n",
    "            steps = (FEATURES, BARCODES, MATRIX, 'combine', ADATA)\n",
    "            \n",
    "            desc = os.path.basename(self.dirname)\n",
    "\n",
    "            steps = tqdm(steps, desc=desc, leave=True)        \n",
    "            for step in steps:\n",
    "                steps.set_postfix(stage=step)\n",
    "                match step:\n",
    "                    case 'features':\n",
    "                        features = pd.read_csv(self.features_filename, sep='\\t', header=None)\n",
    "                        features.columns = [ENSEMBL_ID, GENE_SYMBOL, 'feature_type']\n",
    "                        features.index = pd.Series(features.ensembl_id.copy().values)\n",
    "\n",
    "                    case 'barcodes':\n",
    "                        barcodes = pd.read_csv(self.barcodes_filename, sep='\\t', header=None)\n",
    "                        barcodes.columns = [BARCODES]\n",
    "                        barcodes.index = pd.Series(barcodes.barcodes.copy().values)\n",
    "\n",
    "                    case 'matrix':\n",
    "                        matrix = scprep.io.load_mtx(self.matrix_filename, sparse=True).T\n",
    "\n",
    "                    case 'combine':\n",
    "                        data = pd.DataFrame.sparse.from_spmatrix(\n",
    "                            matrix, columns=features.index, index = barcodes.index\n",
    "                        )\n",
    "                        del matrix\n",
    "\n",
    "                    case 'adata':\n",
    "                        adata = sc.AnnData(X=data.values, obs=barcodes, var=features, dtype='float32')\n",
    "                        adata.write(self.adata_filename)\n",
    "                        \n",
    "                    case _:\n",
    "                        pass\n",
    "\n",
    "            return adata\n",
    "\n",
    "        def get_adata(self) -> AnnData:\n",
    "            adata = sc.read_h5ad(self.adata_filename)\n",
    "            return adata\n",
    "\n",
    "except ImportError as err:\n",
    "    @dataclass\n",
    "    class FilterMatrixDirectory(Directory):\n",
    "        _: KW_ONLY\n",
    "        ADATA_FILE: ClassVar[str] = f'{ADATA}{EXT_H5}'\n",
    "        MATRIX_FILE: ClassVar[str] = f'{MATRIX}{EXT_MTX}'\n",
    "        BARCODES_FILE: ClassVar[str] = f'{BARCODES}{EXT_TSV}'\n",
    "        FEATURES_FILE: ClassVar[str] = f'{FEATURES}{EXT_TSV}'\n",
    "        \n",
    "\n",
    "        def __post_init__(self):    \n",
    "            raise ImportError('FilterMatrixDirectory requires scprep and scanpy to be installed')\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c9665",
   "metadata": {},
   "source": [
    "## Guards\n",
    "> This notebook was generated from [_05_guards.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_05_guards.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68cc91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "from typing import Optional, List, ClassVar, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcf6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import Tensor, Device, SeriesLike, ndarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba2aa075",
   "metadata": {},
   "source": [
    "### Guards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05cda940",
   "metadata": {},
   "source": [
    "#### Numpy and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc042fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_matrix(arr: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `arr` is a np.matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr : SeriesLike\n",
    "        object to check whether or not it is a np.matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(arr, np.matrix)\n",
    "\n",
    "def undo_npmatrix(arr: SeriesLike) -> SeriesLike:  \n",
    "    '''\n",
    "    Given a tensor converts it to a numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor : Tensor\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    arr : ndarray\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - several graphtool functions use dependencies which rely on\n",
    "        the deprecated numpy class `np.matrix`.\n",
    "        \n",
    "    - these functions appear to be related to scipy sparse linalg\n",
    "        methods.\n",
    "    '''\n",
    "    if is_matrix(arr):\n",
    "        return np.array(arr)\n",
    "    return arr\n",
    "\n",
    "def is_series(arr: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `arr` is a pd.Series\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr : SeriesLike\n",
    "        object to check whether or not it is a pd.Series.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(arr, pd.Series)\n",
    "\n",
    "def is_series_like(series_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `series_q` is SeriesLike\n",
    "    i.e. something that is probably data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    series_q : SeriesLike\n",
    "        object to check whether or not it is a SeriesLike.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(series_q, SeriesLike)\n",
    "\n",
    "def is_np(arr_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `arr_q` is a ndarray\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr_q : SeriesLike\n",
    "        object to check whether or not it is a SeriesLike.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(arr_q, ndarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f45a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_device(device_q: Device) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `device_q` is a valid\n",
    "    pytorch device type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    device_q : Device\n",
    "        object to check whether or not it is a pytorch device.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool        \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - There is an execption. `NoneType` is a valid\n",
    "        pytorch device. Here we return `False` instead.\n",
    "    '''\n",
    "    if device_q is None:\n",
    "        return False\n",
    "    return isinstance(device_q, Device)\n",
    "\n",
    "def is_cpu(tensor: Tensor) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `tensor` is on cpu\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor : Tensor\n",
    "        object to check whether or not it is on cpu.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    # assert is_tensor(tensor)\n",
    "    if not hasattr(tensor, 'device'):\n",
    "        return True\n",
    "    return tensor.device.type == 'cpu'\n",
    "\n",
    "def is_mps(tensor: Tensor) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `tensor` is on cpu\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor : Tensor\n",
    "        object to check whether or not it is on cpu.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    # assert is_tensor(tensor)\n",
    "    if not hasattr(tensor, 'device'):\n",
    "        return False\n",
    "    return tensor.device.type == 'mps'\n",
    "\n",
    "def is_tensor(tensor_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Checks whether or not `tensor_q` is a pytorch tensor\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor_q : Tensor\n",
    "        object to check whether or not it is a pytorch tensor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    '''\n",
    "    return isinstance(tensor_q, Tensor)\n",
    "\n",
    "\n",
    "def is_torch(tensor_q: SeriesLike) -> bool:\n",
    "    '''\n",
    "    Alias for `is_tensor`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    tensor_q : Tensor\n",
    "        object to check whether or not it is a pytorch tensor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : bool\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    is_tensor\n",
    "    '''\n",
    "    return is_tensor(tensor_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b979ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def undo_sparse(arr: SeriesLike) -> SeriesLike:  \n",
    "    '''\n",
    "    Given a arr tries to make it a dense array\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr : SeriesLike\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    arr : ndarray\n",
    "    '''\n",
    "    if hasattr(arr, 'toarray'):\n",
    "        arr = arr.toarray()\n",
    "\n",
    "    if hasattr(arr, 'todense'):\n",
    "        arr = arr.todense()\n",
    "\n",
    "    return arr\n",
    "\n",
    "def to_ndarray(arr):\n",
    "    if is_np(arr):\n",
    "        return arr\n",
    "        \n",
    "    arr = undo_npmatrix(arr)\n",
    "    \n",
    "    arr = undo_sparse(arr)\n",
    "\n",
    "    if not is_np(arr):\n",
    "        arr = np.array(arr)\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bef848",
   "metadata": {},
   "source": [
    "## Torch utils\n",
    "> This notebook was generated from [_06_torch_utils.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_06_torch_utils.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6846393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ec74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import Tensor, Device, SeriesLike, ndarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f241d671",
   "metadata": {},
   "source": [
    "### Torch Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    import torch\n",
    "    def ensure_device(device: Device) -> Device:\n",
    "        '''\n",
    "        Given a valid device type attempts to instantiant \n",
    "        a pytorch device object i.e. `device='cpu'` will\n",
    "        return `torch.device('cpu')`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        device : Device\n",
    "            a valid pytorch device type, possible a string.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        device : torch.device        \n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            same error if `torch.device(device)` fails\n",
    "        '''\n",
    "        if device is None:\n",
    "            return device    \n",
    "        try:\n",
    "            return torch.device(device)\n",
    "        except RuntimeError as err:\n",
    "            raise err\n",
    "        return device\n",
    "    \n",
    "    def to_cuda(tensor: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Given a tensor, ensures that it is on cuda.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        return tensor.cuda()\n",
    "\n",
    "    def to_mps(tensor: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Given a tensor, ensures that it is on mac silicon.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        return tensor.to(torch.device('mps'))\n",
    "    \n",
    "    \n",
    "    def to_torch(\n",
    "        arr: SeriesLike,\n",
    "        cuda: Optional[bool] = False,\n",
    "        mps: Optional[bool] = False,\n",
    "        device: Optional[Device] = None,\n",
    "        dtype: Optional[Any] = None\n",
    "    ) -> Tensor:\n",
    "        '''\n",
    "        Given data, ensures that it is a pytorch Tensor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        arr : SeriesLike\n",
    "        \n",
    "        cuda : bool, default=False\n",
    "            whether to return the tensor on cuda\n",
    "            \n",
    "        mps : bool, default=False\n",
    "            whether to return the tensor on mps\n",
    "            \n",
    "        device : Device, optional\n",
    "            whether to return the tensor on given device\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "            the input array as a pytorch tensor\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        - `device` takes priority over `cuda` and `mps`\n",
    "        '''\n",
    "        tensor = torch.as_tensor(arr)\n",
    "        if device is not None:\n",
    "            tensor = tensor.to(device)\n",
    "        elif cuda:\n",
    "            tensor = to_cuda(tensor)\n",
    "        elif mps:\n",
    "            tensor = to_mps(tensor)    \n",
    "        \n",
    "        if dtype is not None:\n",
    "            dtype = coerce_mps_dtype(dtype, tensor.device, assume_on_mps=False)\n",
    "            tensor = tensor.to(dtype)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    #| export\n",
    "    def to_np(tensor:Tensor) -> ndarray:\n",
    "        '''\n",
    "        Given a tensor converts it to a numpy array\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        arr : ndarray\n",
    "        '''\n",
    "        assert is_tensor(tensor)\n",
    "        if not hasattr(tensor, 'detach'):\n",
    "            try:\n",
    "                return np.array(tensor)\n",
    "            except Exception as err:\n",
    "                raise err\n",
    "        try:\n",
    "            return tensor.detach().clone().cpu().numpy()\n",
    "        except Exception as err:\n",
    "            raise err\n",
    "    \n",
    "\n",
    "    def is_mps_available() -> bool:\n",
    "        '''\n",
    "        Checks whether or not pytorch has mps availble (version) and was built with mps in mind.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : bool\n",
    "        '''\n",
    "        maybe_mps = torch.backends.mps.is_available()\n",
    "        built_mps = torch.backends.mps.is_built()\n",
    "        return maybe_mps and built_mps\n",
    "    \n",
    "    def coerce_mps_dtype(\n",
    "        dtype, \n",
    "        device: Optional[Device] = None, \n",
    "        assume_on_mps: Optional[bool] = True\n",
    "    ):\n",
    "        '''\n",
    "        Makes sure `tensor` is `torch.float32` if `tensor.dtype` is `torch.float64`\n",
    "        if `tensor.device` is `'mps'`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        dtype : any\n",
    "            dtype to check against\n",
    "        \n",
    "        device : Device, default=None\n",
    "            the device of the tensor or model from which the `dtype` comes from. If provided\n",
    "            will be used to detemine whether or not to make `torch.float64`, `torch.float32`\n",
    "            only if the device is actually `'mps'`.\n",
    "\n",
    "        assume_on_mps: bool, default=True\n",
    "            whether or not to assume that the device of choice is `'mps'`. Setting this to\n",
    "            `True` will result in `dtype` of `torch.float64` being converted to `torch.float32`\n",
    "            to try and silently fix mps errors\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dtype : any\n",
    "            the dtype, corrected for mps if needed\n",
    "        '''\n",
    "        could_be_mps = is_mps_available()\n",
    "        \n",
    "        is_float64 = dtype == torch.float64\n",
    "\n",
    "        if device is not None:\n",
    "            is_device_mps = device.type == 'mps'\n",
    "            if is_device_mps:\n",
    "                assume_on_mps = True\n",
    "\n",
    "            elif device.type == 'cuda':\n",
    "                assume_on_mps = False\n",
    "\n",
    "        \n",
    "        # NOTE: float64 not availble on mps, coerce to float32\n",
    "        # NOTE: could_be_mps and assume_on_mps both needed as\n",
    "        #       device might not be provided.\n",
    "        if could_be_mps and assume_on_mps and is_float64:\n",
    "            return torch.float32\n",
    "        \n",
    "        return dtype\n",
    "    \n",
    "    def ensure_mps_dtype(tensor: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Makes sure `tensor` is `torch.float32` if `tensor.dtype` is `torch.float64`\n",
    "        if `tensor.device` is `'mps'`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            pytorch tensor to maybe change dtype of\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        dtype = tensor.dtype\n",
    "\n",
    "        # NOTE: we don't assume mps as we explicitly pass the device\n",
    "        dtype = coerce_mps_dtype(dtype, tensor.device, assume_on_mps=False)\n",
    "\n",
    "        tensor = tensor.to(dtype)\n",
    "        return tensor\n",
    "\n",
    "    def move_to(\n",
    "        tensor: Tensor, other: Tensor, \n",
    "        dtype: Optional[Any] = None, do_dtype: Optional[bool] = True\n",
    "    ) -> Tensor:\n",
    "        '''\n",
    "        Makes sure `tensor` is on the same device as `other`\n",
    "        \n",
    "        Parameters\n",
    "        ----------    \n",
    "        tensor : Tensor\n",
    "            pytorch tensor to change device of\n",
    "            \n",
    "        other : Tensor\n",
    "            pytorch tensor we want `tensor` to be on\n",
    "            \n",
    "        dtype : optional\n",
    "            the data type to make `tensor`. If `None` will infer it\n",
    "            from `other`\n",
    "            \n",
    "        do_dype: bool, default=True\n",
    "            whether or not to just match the device of `other` or also\n",
    "            the dtype\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tensor : Tensor\n",
    "        '''\n",
    "        \n",
    "        if not is_tensor(tensor):\n",
    "            tensor = to_torch(tensor)\n",
    "            \n",
    "        # NOTE: dtype not provided, so we will infer it\n",
    "        if dtype is None:\n",
    "            # NOTE: this little line solves mps float64 issues since \n",
    "            #       infer our tensor types and move them accordingly\n",
    "            other = ensure_mps_dtype(other)\n",
    "            dtype = other.dtype\n",
    "\n",
    "        if do_dtype:\n",
    "            tensor = tensor.to(dtype)\n",
    "\n",
    "        tensor = tensor.to(other.device)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "except ImportError as err:\n",
    "    identity = lambda x: x\n",
    "    ensure_device = identity\n",
    "    to_cuda = identity\n",
    "    to_mps = identity\n",
    "    to_torch = lambda arr, cuda, mps, device, dtype: arr\n",
    "    to_np = identity\n",
    "    is_mps_available = lambda: False\n",
    "    coerce_mps_dtype = lambda dtype, device, assume_on_mps: dtype\n",
    "    ensure_mps_dtype = identity\n",
    "    move_to = lambda tensor, other, dtype, do_dtype: tensor\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    import torch, pytorch_lightning as pl\n",
    "    def set_seeds(seed: int) -> None:\n",
    "        '''\n",
    "        Calls a bunch of seed functions with `seed`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int\n",
    "        '''    \n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)    \n",
    "        pl.seed_everything(seed)\n",
    "except ImportError as err:\n",
    "     def set_seeds(seed: int) -> None:\n",
    "         random.seed(seed)\n",
    "         np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a27409",
   "metadata": {},
   "source": [
    "## Adata\n",
    "> This notebook was generated from [_07_adata.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_07_adata.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from typing import List, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e16065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import AnnData, ndarray, DataFrame\n",
    "from iza.static import X_MAGIC, PHATE, X_PHATE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d64b761",
   "metadata": {},
   "source": [
    "### Adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf33a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class AdataExtractor:\n",
    "    adata: AnnData\n",
    "    layer: Optional[str] = X_MAGIC\n",
    "    x_emb: Optional[str] = X_PHATE\n",
    "\n",
    "    dim_str: Optional[str] = None\n",
    "    use_hvg: Optional[bool] = True\n",
    "\n",
    "    @property\n",
    "    def has_hvg(self):\n",
    "        return hasattr(self.adata, 'var') and hasattr(self.adata.var, 'highly_variable')\n",
    "    \n",
    "    @property\n",
    "    def has_emb(self):\n",
    "        return hasattr(self.adata, 'obsm') and self.x_emb in self.adata.obsm.keys()\n",
    "\n",
    "    def get_layer(self) -> ndarray:\n",
    "        layer = self.sdata().layers.get(self.layer, None)\n",
    "\n",
    "        if layer is None:\n",
    "            layer = self.sdata().X\n",
    "\n",
    "        if hasattr(layer, 'toarray'):\n",
    "            layer = layer.toarray()\n",
    "\n",
    "        if hasattr(layer, 'todense'):\n",
    "            layer = layer.todense()\n",
    "\n",
    "        return layer\n",
    "    \n",
    "    def get_emb(self) -> ndarray:\n",
    "        emb = self.sdata().obsm.get(self.x_emb, None)\n",
    "        if emb is None:\n",
    "            raise ValueError(f'No embedding found in adata.obsm {self.sdata().obsm.keys()}')\n",
    "\n",
    "        # NOTE: defined in _02_utils/_05_guards.ipynb\n",
    "        emb = to_ndarray(emb)\n",
    "        return emb\n",
    "\n",
    "    @property\n",
    "    def axis_str(self):\n",
    "        if self.dim_str:\n",
    "            return self.dim_str\n",
    "        return self.x_emb.replace('X_', '').upper()\n",
    "    \n",
    "    @property\n",
    "    def emb_cols(self):\n",
    "        ndim = self.get_emb().shape[1]\n",
    "        cols = [f'{self.axis_str}_{i+1}' for i in range(ndim)]\n",
    "        return cols\n",
    "        \n",
    "    def sdata(self):\n",
    "        if self.use_hvg and self.has_hvg:\n",
    "            return self.adata[:, self.adata.var.highly_variable]\n",
    "        return self.adata\n",
    "    \n",
    "    def get_df_cnt(self) -> DataFrame:\n",
    "        layer = self.get_layer()\n",
    "\n",
    "        cols = self.sdata().var.index\n",
    "        idxs = self.sdata().obs.index\n",
    "        df = pd.DataFrame(layer, index=idxs, columns=cols)\n",
    "        return df\n",
    "    \n",
    "    def get_df_emb(self) -> DataFrame:\n",
    "        emb = self.get_emb()\n",
    "        \n",
    "        cols = self.emb_cols\n",
    "        idxs = self.sdata().obs.index\n",
    "        df = pd.DataFrame(emb, index=idxs, columns=cols)\n",
    "        return df\n",
    "    \n",
    "    @property\n",
    "    def df_cnt(self):\n",
    "        return self.get_df_cnt()\n",
    "    \n",
    "    @property\n",
    "    def df_emb(self):\n",
    "        return self.get_df_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b70396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77f8f7e0",
   "metadata": {},
   "source": [
    "## Ex\n",
    "> This notebook was generated from [_09_exp.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_09_exp.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, yaml, datetime, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72907394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def config_exp_logger(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        logger\n",
    "    '''\n",
    "    basename = os.path.basename(path)\n",
    "    logger = logging.getLogger(basename)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=exp_log_filename(path), \n",
    "        encoding='utf-8',\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s\\t%(levelname)s:%(message)s',\n",
    "        datefmt='%d/%m/%Y %I:%M:%S %p',\n",
    "        filemode='w'\n",
    "    )\n",
    "    logger.info(f'Experiment path created {path}')\n",
    "    return logger\n",
    "\n",
    "def exp_log_filename(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        log_file (str): full path to provided experiment's log file\n",
    "    '''\n",
    "    return os.path.join(path, 'log.txt')\n",
    "\n",
    "def exp_param_filename(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        param_file (str): full path to provided experiment's parameter file\n",
    "    '''\n",
    "    return os.path.join(path, 'params.yml')\n",
    "\n",
    "def list_exps(path):\n",
    "    '''\n",
    "    Notes:\n",
    "    ----------\n",
    "        - an experiment is defined as a directory containing a `'params.yml'` file.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiments directory, i.e. \n",
    "            `<path-to-experiments-dir>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        experiments (str[]): experiments (subdirectories) in the specified \n",
    "            `path`.\n",
    "    '''\n",
    "    test_fn = lambda el: os.path.isdir(el) and os.path.isfile(exp_param_filename(el))\n",
    "    return list(filter(test_fn, os.listdir(path)))\n",
    "\n",
    "def gen_exp_name(name=None):\n",
    "    '''    \n",
    "    Returns:\n",
    "    ----------\n",
    "        exp_name (str): timestamp to serve as experiment name\n",
    "    '''\n",
    "    if name is None:\n",
    "        now =  datetime.datetime.now()\n",
    "        return now.strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "    return name\n",
    "        \n",
    "def load_exp_params(path):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "    Returns:\n",
    "    ----------\n",
    "        params (dict): the loaded parameters\n",
    "    '''\n",
    "    with open(exp_param_filename(path)) as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "\n",
    "def save_exp_params(path, params, logger=None):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "        params (dict): dictionary of parameters to save\n",
    "        logger (logging.Logger): Defaults to None.\n",
    "    '''\n",
    "    with open(exp_param_filename(path), 'w') as f:\n",
    "        yaml.dump(params, f, default_flow_style=False)\n",
    "    if logger: \n",
    "        logger.info('Experiment parameters saved.')\n",
    "\n",
    "def setup_exp(path, params, name=None):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to where to create experiments, i.e. \n",
    "            `<path-to-experiments-dir>`\n",
    "        params (dict): dictionary of parameters to save\n",
    "    Returns:\n",
    "    ----------\n",
    "        exp_dir (str): full path to experiment, i.e. \n",
    "            `<path-to-experiments-dir>/<experiment-timestamp>`\n",
    "        logger (logging.Logger)\n",
    "    '''\n",
    "    exp_name = gen_exp_name(name)\n",
    "    exp_dir = os.path.join(path, exp_name)\n",
    "    if not os.path.isdir(exp_dir):\n",
    "        os.makedirs(exp_dir)\n",
    "\n",
    "    logger = config_exp_logger(exp_dir)    \n",
    "    save_exp_params(exp_dir, params, logger)\n",
    "    return exp_dir, logger\n",
    "\n",
    "    \n",
    "def is_config_subset(truth, params):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        truth (dict): dictionary of parameters to compare to\n",
    "        params (dict): dictionary of parameters to test\n",
    "    Returns:\n",
    "    ----------\n",
    "        result (bool) whether or not `params` is a subset of `truth`\n",
    "    '''\n",
    "    if not type(truth) == type(params): return False\n",
    "    for key, val in params.items():\n",
    "        if key not in truth: return False\n",
    "        if type(val) is dict:\n",
    "            if not is_config_subset(truth[key], val): \n",
    "                return False\n",
    "        else:            \n",
    "            if not truth[key] == val: return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def find_exps(path, params):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        path (str): full path to where to create experiments, i.e. \n",
    "            `<path-to-experiments-dir>`\n",
    "        params (dict): dictionary of parameters to test\n",
    "    Returns:\n",
    "    ----------\n",
    "        results (str[]) list of experiment names where their parameters are \n",
    "            supersets of the provided `params`\n",
    "    '''\n",
    "    exps = list_exps(path)\n",
    "    results = []\n",
    "    for exp in exps:\n",
    "        exp_name = os.path.join(path, exp)\n",
    "        exp_params = load_exp_params(exp_param_filename(exp_name))\n",
    "        if is_config_subset(exp_params, params):\n",
    "            results.append(exp)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4d7fc",
   "metadata": {},
   "source": [
    "## Archive\n",
    "> This notebook was generated from [_10_archive.ipynb](/Users/solst/Projects/iza/nbs/_02_utils/_10_archive.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c24848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, pathlib, itertools\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, KW_ONLY\n",
    "from typing import Optional, List, ClassVar, Any, TypeAlias, Union\n",
    "\n",
    "from ipos.imp import is_mod, is_var_imp, Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from iza.types import (\n",
    "    PathLike, PathType,\n",
    "    RichConsole, RichProgress, RichText, RichTree\n",
    ")\n",
    "from iza.static import EXT_PY\n",
    "from iza.imp import RichImp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eed96fc",
   "metadata": {},
   "source": [
    "### Directory Viewer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "734155c9",
   "metadata": {},
   "source": [
    "#### Archive Downloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab40bc9e",
   "metadata": {},
   "source": [
    "- `Directory` defined in `_02_utils/_03_directory.ipynb`\n",
    "- `ConsoleType` defined in `_02_utils/_03_directory.ipynb`\n",
    "- `get_console` imported in `_02_utils/_03_directory.ipynb`\n",
    "- `is_rich_available` defined in `_02_utils/_08_archive.ipynb`\n",
    "- `urljoin` defined in `_02_utils/_01_files.ipynb`\n",
    "- `parse_url` imported in `_02_utils/_01_files.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d854be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ArchiveDownloader:    \n",
    "    _: KW_ONLY\n",
    "    rootdir: str\n",
    "    archive: str\n",
    "    entries: Union[str, list[str]]\n",
    "    savedir: str\n",
    "    extract: bool = False\n",
    "    cleanup: bool = False\n",
    "    compound_archive: bool = False\n",
    "    archives: Optional[list[str]] = None\n",
    "    console: Optional[RichConsole] = None\n",
    "    progress: Optional[RichProgress] = None\n",
    "\n",
    "    _rich: Module = field(init=False, repr=False, default=None)\n",
    "    has_rich: bool = field(init=False, repr=False, default=None)\n",
    "\n",
    "    \n",
    "\n",
    "    def __post_init__(self):    \n",
    "        r = RichImp()\n",
    "        self._rich = r._module\n",
    "        self.has_rich = is_mod(self._rich)\n",
    "\n",
    "        self.entries = self.entries if isinstance(self.entries, list) else [self.entries]        \n",
    "        self.console = get_console()        \n",
    "        self.progress = self.get_progress()\n",
    "    \n",
    "        self.savedir = Path(self.savedir).expanduser()\n",
    "        make_missing_dirs(self.savedir)\n",
    "\n",
    "    def get_progress(self):\n",
    "        progress = getattr(self, 'progress', None)\n",
    "        if progress is not None:\n",
    "            return progress\n",
    "\n",
    "        if self.has_rich and is_var_imp('Progress'):\n",
    "                self.progress = Progress(console=self.console)\n",
    "                return self.progress\n",
    "\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        return urljoin(self.rootdir, self.archive)\n",
    "\n",
    "    @property\n",
    "    def urls(self) -> list[str]:\n",
    "        urls = []\n",
    "        if self.compound_archive and self.archives is not None:\n",
    "            for archive, entry in itertools.product(self.archives, self.entries):\n",
    "                urls.append(urljoin(self.rootdir, archive, entry))\n",
    "        else:\n",
    "            urls = [urljoin(self.path, entry) for entry in self.entries]\n",
    "        return urls\n",
    "\n",
    "    def download_missing_files(self) -> None:        \n",
    "        total_files = len(self.urls)\n",
    "        if self.has_rich and self.progress:\n",
    "            with self.progress:\n",
    "                task = self.progress.add_task(\"[cyan]Downloading...\", total=total_files)\n",
    "                for url in self.urls:\n",
    "                    filename = Path(parse_url(url).path).name\n",
    "                    fullpath = self.savedir / filename\n",
    "                    if not fullpath.exists():\n",
    "                        stream_file(url, str(fullpath))\n",
    "                        self.progress.advance(task)\n",
    "                    else:\n",
    "                        self.progress.advance(task)\n",
    "        else:            \n",
    "            for url in tqdm(self.urls, desc='Downloading'):       \n",
    "                filename = Path(parse_url(url).path).name\n",
    "                fullpath = self.savedir / filename\n",
    "                if not fullpath.exists():\n",
    "                    stream_file(url, str(fullpath))\n",
    "                \n",
    "\n",
    "    def calc_n_to_extract(self) -> int:\n",
    "        files = [self.savedir / entry for entry in self.entries]\n",
    "        files = get_gz_files_in_dir(self.savedir)\n",
    "        total = 0\n",
    "        for file in files:\n",
    "            if is_tarball(file):\n",
    "                total += 1\n",
    "            elif is_gz(file):\n",
    "                total += 1\n",
    "        return total                \n",
    "\n",
    "    def extract_files(self) -> None:\n",
    "        recurser = RecursiveDecompressor(\n",
    "            dirname=self.savedir, \n",
    "            entries=self.entries,\n",
    "            strategy=undo_gz,\n",
    "            remove=self.cleanup,             \n",
    "            progress=self.progress\n",
    "        )\n",
    "        recurser.decompress()\n",
    "        return\n",
    "        files = [self.savedir / entry for entry in self.entries]\n",
    "        files = get_gz_files_in_dir(self.savedir)\n",
    "        total = self.calc_n_to_extract()\n",
    "        if self.has_rich and self.progress:\n",
    "            with self.progress:\n",
    "                task = self.progress.add_task(\"[cyan]Extracting...\", total=total)\n",
    "                for file in files:\n",
    "                    undo_gz(file, remove=self.cleanup)                    \n",
    "                    self.progress.advance(task)\n",
    "        else:\n",
    "            for file in tqdm(files, desc='Extracting'):\n",
    "                undo_gz(file, remove=self.cleanup)\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        if self.has_rich and self.console:\n",
    "            self.console.print(f\"Processing archive: [bold cyan]{self.archive}[/bold cyan]\")\n",
    "        else:\n",
    "            print(f\"Processing archive: {self.archive}\")\n",
    "\n",
    "        self.download_missing_files()\n",
    "        if self.extract:\n",
    "            self.extract_files()\n",
    "\n",
    "        if self.has_rich and self.console:\n",
    "            dir = RichDirectory(self.savedir, console=self.console)\n",
    "            dir.print_rich()\n",
    "        else:\n",
    "            dir = Directory(self.savedir)\n",
    "            dir.print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c662d3a5",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8509ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: False\n",
    "from iza.static import AMAZON_BUCKET_FLUENTBIO\n",
    "\n",
    "downloader = ArchiveDownloader(\n",
    "    rootdir = AMAZON_BUCKET_FLUENTBIO,\n",
    "    archive = 'public-datasets/pbmc/',\n",
    "    entries = ['combined.html', 'filtered_matrix.tar.gz'],\n",
    "    savedir = '~/Downloads/fluent_bio',  extract=True, cleanup=True\n",
    ")\n",
    "downloader.execute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "412ca255",
   "metadata": {},
   "source": [
    "#### Typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: False\n",
    "#| export\n",
    "try:\n",
    "    import typer\n",
    "    app = typer.Typer()\n",
    "\n",
    "    @app.command()\n",
    "    def download(rootdir: str, archive: str, entries: List[str], savedir: str, extract: bool = False, cleanup: bool = False):\n",
    "        downloader = ArchiveDownloader(rootdir, archive, entries, savedir, extract, cleanup)\n",
    "        downloader.execute()\n",
    "\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7df159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
